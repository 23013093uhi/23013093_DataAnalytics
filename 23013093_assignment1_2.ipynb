{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQqUn2lVlDfb35zZP+9Pyq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/23013093uhi/23013093_DataAnalytics/blob/main/23013093_assignment1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Corinn Sinnott\n",
        "###23013093\n",
        "###Assignment1-2"
      ],
      "metadata": {
        "id": "jb5S82kn4ovP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p-7cPGENClE"
      },
      "source": [
        "#load packages\n",
        "# pandas is used to create the data frame (McKinney et al., 2010)\n",
        "import pandas as pd\n",
        "\n",
        "# numpy is used for calculations (Harris et al., 2020)\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorflow is used for machine learning (Abadi et al., 2015)\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "tf.keras.utils.set_random_seed(43)  # sets seed for reproducibility, remove to get a different run - results may vary\n",
        "tf.config.experimental.enable_op_determinism()"
      ],
      "metadata": {
        "id": "5DzJzm8VY1rG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "I have been tasked by the New York City Emergency Services Unit with investigating the relationship between weather conditions, day of week, and number of traffic collisions. Previously, I explored NOAA weather data and NYPD motor vehicle collision data in order to study this relationship.\n",
        "The analysis showed a positive correlation between number of collisions and:\n",
        "*   mean temperature\n",
        "*   fog\n",
        "*   mean dewpoint\n",
        "*   maximum temperature\n",
        "*   minimum temperature\n",
        "\n",
        "It also showed a negative correlation between number of collisions and:\n",
        "*   mean visibility\n",
        "*   mean wind speed\n",
        "*   maximum sustained wind speed\n",
        "*   maximum wind gust\n",
        "\n",
        "The best fitting linear model included:\n",
        "*   mean temperature\n",
        "*   mean dewpoint\n",
        "*   mean visibility\n",
        "*   mean wind speed\n",
        "*   maximum sustained wind speed\n",
        "*   maximum wind gust\n",
        "*   total precipitation\n",
        "*   day of week\n",
        "\n",
        "The aim of this report is be able to predict how many traffic collisions are likely to occur on a particular day. In order to do this, I will build both linear regression models and deep neural network (DNN) models to try to predict the number of collisions on a particular day based on available data. The goal is to develop a prediction model to assist the New York City Emergency Services Unit with planning and optimization, ensuring that resources are directed to where they are most needed."
      ],
      "metadata": {
        "id": "0DOBf0H-ieRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "I used the previously gathered, collated, and cleaned data from the NOAA Global Surface Summary of the Day (noaa_gsod) (NOAA National Centers of Environmental Information, 1999) and NYPD Motor Vehicle Collisions (new_york_mv_collisions) (New York Police Department, 2017) datasets. Github link: https://raw.githubusercontent.com/23013093uhi/23013093_DataAnalytics/main/collated_collision_data_clean.csv\n",
        "\n",
        "\n",
        "Both linear regession and deep neural network (DNN) models were trained and tested on the data. I created a dataframe from the .csv file and shuffled the data in order to prevent bias. For each set of variables tested, I created an input dataframe from the shuffled data and removed rows with NA values, as the regression cannot handle them. I created a training set consisting of 80% of the data and a test set of the remaining 20%. I also created a validation set (5%). I used the training set to train the model and the test set to test it. The mean absolute error was used to evaluate and compare models due to the amount of outliers. I also used the validation set to compare actual and predicted values to further assess the models.\n",
        "\n",
        "##Linear Modelling\n",
        "I created a linear regression model using all of the variables as well as one using only the variables previously determined to be significant when fitting a linear model:\n",
        "* mean temperature\n",
        "* mean dewpoint\n",
        "* mean visibility\n",
        "* mean wind speed\n",
        "* maximum sustained wind speed\n",
        "* maximum wind gust\n",
        "* total precipitation\n",
        "* day of week\n",
        "\n",
        "I also tried removing the significant variables one by one and creating linear regression models to see if mean absolute error could be further reduced.\n",
        "\n",
        "##DNN Modelling\n",
        "I created a DNN model using all of the variables. Next, I tried removing the variables one by one and creating DNN models to see if mean absolute error could be reduced. I then tried taking all of the variables that when removed individually reduced mean absolute error and removed them all from the model. I also repeated this process while increasing the number of neurons in the dense layer from 48 to 100 to see if mean absolute error could be further reduced. I also tried increasing the number of layers in the DNN from 2 to 3. Finally, I tried creating a DNN model with only the significant variables from the linear model.\n"
      ],
      "metadata": {
        "id": "-WJbURQ0pYtg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLj5jwHF4oLg",
        "outputId": "8f18cb13-47f8-423d-ee88-c1298b769fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   day  year  mo  da collision_date  temp  dewp     slp  visib  wdsp  mxpsd  \\\n",
            "1    7  2018   9  23     2018-09-23  59.8  50.2  1023.4   10.0   3.0    5.1   \n",
            "2    6  2021  11   6     2021-11-06  43.1  29.7  1028.9   10.0   3.1    6.0   \n",
            "3    5  2019   3   1     2019-03-01  26.7  18.2  1026.9   10.0   4.0    7.0   \n",
            "4    4  2018  12  20     2018-12-20  38.6  34.4  1020.2    9.6   5.0    7.0   \n",
            "5    1  2019   7   8     2019-07-08  64.9  56.7  1014.7   10.0   4.2    7.0   \n",
            "6    1  2024   7   8     2024-07-08  72.0  71.5  1016.6    4.0   4.2    7.0   \n",
            "\n",
            "   gust   max   min  prcp  sndp  fog  NUM_COLLISIONS  \n",
            "1   NaN  78.1  53.1   0.0   NaN    0             475  \n",
            "2   NaN  55.0  32.0   0.0   NaN    0             335  \n",
            "3   NaN  35.1  16.0   0.0   NaN    0             711  \n",
            "4   NaN  48.0  21.0   0.0   NaN    0             806  \n",
            "5   NaN  75.0  54.0   0.0   NaN    0             592  \n",
            "6   NaN  78.1  68.0   0.0   NaN    1             238  \n"
          ]
        }
      ],
      "source": [
        "# create dataframe from previously created csv file of clean collated weather collison data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/23013093uhi/23013093_DataAnalytics/main/collated_collision_data_clean.csv', index_col=0, )\n",
        "\n",
        "#print the first 6 lines to check the data\n",
        "print(df[:6])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the data to randomise it in order to prevent bias\n",
        "\n",
        "#take a sample of the entire dataset (fraction = 1) thereby shuffling it. Set random_state = 0 for reproducibility\n",
        "df_shuffle = df.sample(frac=1, random_state=0)\n",
        "\n",
        "#print first 6 lines to check data\n",
        "print(df_shuffle[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQB7driPQHUn",
        "outputId": "fe3bbd2e-e9df-4a98-d181-fcde7f1ef8e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  year  mo  da collision_date  temp  dewp     slp  visib  wdsp  \\\n",
            "1785    3  2019   6  12     2019-06-12  62.9  51.9  1019.8   10.0   8.2   \n",
            "2807    7  2020   6   7     2020-06-07  62.2  56.2  1007.9    8.8  12.3   \n",
            "2476    5  2016   7   1     2016-07-01  66.9  63.6  1017.1    7.8   8.7   \n",
            "2288    3  2019   6   5     2019-06-05  59.8  55.0  1014.3    9.6  12.3   \n",
            "1697    2  2018   6  12     2018-06-12  55.3  48.6  1022.7   10.0   9.6   \n",
            "640     1  2017  12  18     2017-12-18  32.7  23.9  1021.3    9.7   4.6   \n",
            "\n",
            "      mxpsd  gust   max   min  prcp  sndp  fog  NUM_COLLISIONS  \n",
            "1785   15.0   NaN  73.9  57.0  0.57   NaN    0             686  \n",
            "2807   18.1  22.9  66.0  57.9  0.01   NaN    1             206  \n",
            "2476   17.1   NaN  77.0  61.0  0.00   NaN    1             752  \n",
            "2288   15.9  21.0  66.0  51.1  0.00   NaN    0             655  \n",
            "1697   15.0   NaN  64.0  46.0  0.00   NaN    0             720  \n",
            "640    11.1   NaN  43.0  24.1  0.00   NaN    0             706  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTMxwMfGPdWJ"
      },
      "source": [
        "# create a scale - I used 1200 based on the maximum value of NUM_COLLISIONS\n",
        "SCALE_NUM_COLLISIONS = 1200"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this run, the lowest mean absolute error was achieved by using the linear model with all significant variables:\n",
        "*   mean temperature\n",
        "*   mean dewpoint\n",
        "*   mean visibility\n",
        "*   mean wind speed\n",
        "*   maximum sustained wind speed\n",
        "*   maximum wind gust\n",
        "*   total precipitation\n",
        "*   day of week\n",
        "\n",
        "The predicted number of collisions also appears reasonable compared to the actual values. I have set the seed so that the result shown will match the write-up. The results can vary if the seed is not set, but every time I ran the models the lowest mean absolute error was achieved by using one of the linear models with some or all of these significant variables. The DNN model with the lowest mean absolute error was achieved by removing all of the variables whose individual removal lowered the mean absolute error. Increasing either the number of layers or the number of neurons in the DNN model did not further decrease mean absolute error. The mean absolute error from the best DNN model was still higher than that obtained from the linear model with all significant variables.\n"
      ],
      "metadata": {
        "id": "cy0H_V9upY3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All Variables"
      ],
      "metadata": {
        "id": "40c-Haw3t_g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers = [\"day\", \"temp\", \"dewp\", \"slp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"max\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input = pd.concat(df_input_data, axis=1, keys=df_input_headers)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input = df_input.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input[:6])"
      ],
      "metadata": {
        "id": "WHuSnr6LdC3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4eb9d44-d497-4625-996c-6e26e1ec5a5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  \\\n",
            "2807    7  62.2  56.2  1007.9    8.8  12.3   18.1  22.9  66.0  57.9  0.01   \n",
            "2288    3  59.8  55.0  1014.3    9.6  12.3   15.9  21.0  66.0  51.1  0.00   \n",
            "3995    2  48.3  45.7  1010.6    3.4  17.3   25.1  35.0  54.0  41.0  0.57   \n",
            "2333    1  37.8  31.6  1018.6    9.9  13.3   17.1  21.0  46.0  25.0  0.00   \n",
            "4127    4  37.7  24.6  1017.9    8.2  19.9   27.0  42.9  51.1  30.0  0.24   \n",
            "2973    1  68.3  64.1  1020.1   10.0  11.2   19.0  22.0  75.0  50.0  0.00   \n",
            "\n",
            "      fog  NUM_COLLISIONS  \n",
            "2807    1             206  \n",
            "2288    0             655  \n",
            "3995    1             608  \n",
            "2333    0             238  \n",
            "4127    0             252  \n",
            "2973    0             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set = df_input.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set = df_input.drop(training_set.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set = df_input.sample(frac=0.05, random_state=0)"
      ],
      "metadata": {
        "id": "d5JtvdLBcruq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a copy of the datasets\n",
        "training_features = training_set.copy()\n",
        "valid_features = valid_set.copy()\n",
        "test_features = test_set.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels = training_features.pop('NUM_COLLISIONS')\n",
        "test_labels = test_features.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features.pop('NUM_COLLISIONS')"
      ],
      "metadata": {
        "id": "EILvYoDid0wI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "6409bad7-93af-4103-82ca-7ed7cd28c1d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1504    181\n",
              "2408    519\n",
              "3567    472\n",
              "1720    279\n",
              "2977    328\n",
              "       ... \n",
              "4357    233\n",
              "3720    432\n",
              "883     480\n",
              "1273    474\n",
              "3173    666\n",
              "Name: NUM_COLLISIONS, Length: 136, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1504</th>\n",
              "      <td>181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2408</th>\n",
              "      <td>519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3567</th>\n",
              "      <td>472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1720</th>\n",
              "      <td>279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2977</th>\n",
              "      <td>328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4357</th>\n",
              "      <td>233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3720</th>\n",
              "      <td>432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1273</th>\n",
              "      <td>474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3173</th>\n",
              "      <td>666</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>136 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divide by the scale factor (1200) to scale the data\n",
        "training_labels = training_labels/SCALE_NUM_COLLISIONS\n",
        "test_labels = test_labels/SCALE_NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "3NBSa-Wyf2r6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the training set training_features for the normalisation layer in order to try and fit to the output\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features))"
      ],
      "metadata": {
        "id": "VQuNfPpdf9al"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model and add normaliser\n",
        "model_0 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])"
      ],
      "metadata": {
        "id": "JKiP5GIrgIJH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sequential model, with designated optimiser and loss parameter mean absolute error\n",
        "model_0.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "metadata": {
        "id": "4odNLaUHgNb7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use training features and labels to fit the model. Run 100 times and include 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_0.fit(\n",
        "    training_features,\n",
        "    training_labels,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "id": "rW1K3GxUgQyC",
        "outputId": "5fca085b-8724-4983-a849-c1f38f944920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14.1 s, sys: 716 ms, total: 14.8 s\n",
            "Wall time: 17.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model using the test features and labels. Obtain mean absolute error\n",
        "mean_absolute_error_model_0 = model_0.evaluate(\n",
        "    test_features,\n",
        "    test_labels, verbose=0)"
      ],
      "metadata": {
        "id": "ALdhLDb_gSjq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the mean absolute error of the model\n",
        "print(mean_absolute_error_model_0)"
      ],
      "metadata": {
        "id": "NVLVxc_aghcD",
        "outputId": "77154dff-9f6a-445a-dcc2-daa09226f92d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1782083511352539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model on the validation set and compare predicted with actual\n",
        "linear_day_predictions_0 = model_0.predict(valid_features)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set0 = valid_set.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set0[\"NUM_COLLISIONS_predicted\"] = linear_day_predictions_0.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1qTFSil0r1-",
        "outputId": "1364c7f5-0b7e-4788-bcc1-f2f8564036eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  \\\n",
            "1504    6  41.2  37.6  1017.1    9.1   7.5   14.0  20.0  48.9  37.9  0.10   \n",
            "2408    7  69.4  59.9  1014.4    9.5  11.8   17.1  22.9  75.0  64.0  0.00   \n",
            "3567    7  39.4  34.8  1021.0    8.3  12.4   21.0  29.9  45.0  33.1  0.55   \n",
            "1720    4  61.0  61.0  1005.3    4.2   8.6   15.0  22.0  66.9  54.0  0.03   \n",
            "2977    6  61.4  58.0  1019.9    6.5  11.1   19.0  28.0  66.9  52.0  0.08   \n",
            "...   ...   ...   ...     ...    ...   ...    ...   ...   ...   ...   ...   \n",
            "4357    4  38.9  36.3  1010.4    4.0  22.0   34.0  45.1  46.9  26.1  0.20   \n",
            "3720    7  56.5  48.2  1026.6   10.0  15.5   22.0  25.1  63.0  51.1  0.00   \n",
            "883     7  63.8  50.3  1015.4    9.0   6.4   12.0  15.9  72.0  55.4  0.22   \n",
            "1273    6  75.7  65.2  1023.6    7.3   3.2   13.0  18.1  84.0  62.6  0.01   \n",
            "3173    4  30.9  10.8  1015.8   10.0  14.5   20.0  27.0  37.0  25.0  0.00   \n",
            "\n",
            "      fog  NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504    0             181      [584.7384033203125]  \n",
            "2408    0             519     [462.28533935546875]  \n",
            "3567    0             472      [524.3594360351562]  \n",
            "1720    1             279              [317.78125]  \n",
            "2977    0             328      [487.7968444824219]  \n",
            "...   ...             ...                      ...  \n",
            "4357    0             233      [142.3376922607422]  \n",
            "3720    0             432      [607.8272094726562]  \n",
            "883     0             480      [571.0745849609375]  \n",
            "1273    1             474        [768.24560546875]  \n",
            "3173    0             666      [457.3376770019531]  \n",
            "\n",
            "[136 rows x 14 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All Significant Variables"
      ],
      "metadata": {
        "id": "ftUaNszMuGMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "#get the data\n",
        "df_input_data1 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers1 = [\"day\", \"temp\", \"dewp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input1 = pd.concat(df_input_data1, axis=1, keys=df_input_headers1)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input1 = df_input1.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input1[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set1 = df_input1.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set1 = df_input1.drop(training_set1.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set1 = df_input1.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features1 = training_set1.copy()\n",
        "valid_features1 = valid_set1.copy()\n",
        "test_features1 = test_set1.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels1 = training_features1.pop('NUM_COLLISIONS')\n",
        "test_labels1 = test_features1.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features1.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels1 = training_labels1/SCALE_NUM_COLLISIONS\n",
        "test_labels1 = test_labels1/SCALE_NUM_COLLISIONS\n",
        "\n",
        "# use the training set training_features for the normalisation layer in order to try and fit to the output\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features1))\n",
        "\n",
        "# create model and add normaliser\n",
        "model_1 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# create a sequential model, with designated optimiser and loss parameter mean absolute error\n",
        "model_1.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC0wsGFlCBiE",
        "outputId": "e4fcd28f-88fb-444b-d645-d65a8ef11252"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  visib  wdsp  mxpsd  gust  NUM_COLLISIONS\n",
            "2807    7  62.2  56.2    8.8  12.3   18.1  22.9             206\n",
            "2288    3  59.8  55.0    9.6  12.3   15.9  21.0             655\n",
            "3995    2  48.3  45.7    3.4  17.3   25.1  35.0             608\n",
            "2333    1  37.8  31.6    9.9  13.3   17.1  21.0             238\n",
            "4127    4  37.7  24.6    8.2  19.9   27.0  42.9             252\n",
            "2973    1  68.3  64.1   10.0  11.2   19.0  22.0             521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use training features and labels to fit the model. Run 100 times and include 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_1.fit(\n",
        "    training_features1,\n",
        "    training_labels1,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVQpaPJ3CB3k",
        "outputId": "5efbe674-6e9e-4f9c-a780-1a0f1c46d6f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.6 s, sys: 680 ms, total: 14.2 s\n",
            "Wall time: 16.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model using the test features and labels. Obtain mean absolute error\n",
        "mean_absolute_error_model_1 = model_1.evaluate(\n",
        "    test_features1,\n",
        "    test_labels1, verbose=0)\n",
        "\n",
        "# Print the mean absolute error of the model\n",
        "print(mean_absolute_error_model_1)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "linear_day_predictions_1 = model_1.predict(valid_features1)*1200 # scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set1a = valid_set1.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set1a[\"NUM_COLLISIONS_predicted\"] = linear_day_predictions_1.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set1a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyEX6C2qCB-2",
        "outputId": "f228cba8-8f53-46e1-dd2e-f6ff6d90c48d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1279795616865158\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "      day  temp  dewp  visib  wdsp  mxpsd  gust  NUM_COLLISIONS  \\\n",
            "3271    1  62.3  58.2    8.8  13.8   20.0  27.0             641   \n",
            "3709    7  48.8  39.8    9.6  11.9   22.0  28.9             241   \n",
            "3135    7  49.6  47.9    6.0  15.3   19.0  28.0             256   \n",
            "4263    4  43.7  39.1    5.3  16.4   28.9  36.9             266   \n",
            "1746    4  42.7  35.1    9.8   6.4   15.0  22.9             635   \n",
            "...   ...   ...   ...    ...   ...    ...   ...             ...   \n",
            "2771    2  63.3  59.7    8.7  14.1   18.1  22.9             318   \n",
            "2355    2  37.1  22.6   10.0  12.2   17.1  24.1             282   \n",
            "2506    5  53.6  44.3   10.0  12.1   17.1  22.9             777   \n",
            "3595    7  37.8  34.1    9.1  13.4   22.0  28.0             541   \n",
            "2922    3  42.1  38.5    5.0  12.3   19.0  25.1             565   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3271      [538.3699340820312]  \n",
            "3709     [504.58038330078125]  \n",
            "3135     [404.02838134765625]  \n",
            "4263      [460.0755920410156]  \n",
            "1746      [558.6099853515625]  \n",
            "...                       ...  \n",
            "2771      [533.5206909179688]  \n",
            "2355      [543.6273803710938]  \n",
            "2506      [524.2052612304688]  \n",
            "3595      [471.0416564941406]  \n",
            "2922       [490.929931640625]  \n",
            "\n",
            "[137 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Mean Temperature"
      ],
      "metadata": {
        "id": "R6UVilMLEwwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "#get the data\n",
        "df_input_data2 = [df_shuffle[\"day\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers2 = [\"day\", \"dewp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input2 = pd.concat(df_input_data2, axis=1, keys=df_input_headers2)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input2 = df_input2.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input2[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set2 = df_input2.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set2 = df_input2.drop(training_set2.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set2 = df_input2.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features2 = training_set2.copy()\n",
        "valid_features2 = valid_set2.copy()\n",
        "test_features2 = test_set2.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels2 = training_features2.pop('NUM_COLLISIONS')\n",
        "test_labels2 = test_features2.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features2.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels2 = training_labels2/SCALE_NUM_COLLISIONS\n",
        "test_labels2 = test_labels2/SCALE_NUM_COLLISIONS\n",
        "\n",
        "# use the training set training_features for the normalisation layer in order to try and fit to the output\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features2))\n",
        "\n",
        "# create model and add normaliser\n",
        "model_2 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# create a sequential model, with designated optimiser and loss parameter mean absolute error\n",
        "model_2.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IHitEgsFtNz",
        "outputId": "6c9698dd-e9d6-4f50-810e-09b406155f0f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  dewp  visib  wdsp  mxpsd  gust  NUM_COLLISIONS\n",
            "2807    7  56.2    8.8  12.3   18.1  22.9             206\n",
            "2288    3  55.0    9.6  12.3   15.9  21.0             655\n",
            "3995    2  45.7    3.4  17.3   25.1  35.0             608\n",
            "2333    1  31.6    9.9  13.3   17.1  21.0             238\n",
            "4127    4  24.6    8.2  19.9   27.0  42.9             252\n",
            "2973    1  64.1   10.0  11.2   19.0  22.0             521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use training features and labels to fit the model. Run 100 times and include 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_2.fit(\n",
        "    training_features2,\n",
        "    training_labels2,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rihrVn7gFtWj",
        "outputId": "0f708981-90fb-42a2-a59c-986f04ecd755"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.3 s, sys: 672 ms, total: 14 s\n",
            "Wall time: 14.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model using the test features and labels. Obtain mean absolute error\n",
        "mean_absolute_error_model_2 = model_2.evaluate(\n",
        "    test_features2,\n",
        "    test_labels2, verbose=0)\n",
        "\n",
        "# Print the mean absolute error of the model\n",
        "print(mean_absolute_error_model_2)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "linear_day_predictions_2 = model_2.predict(valid_features2)*1200 # essentially 600000 in this instance would give back realistic numbers based on the TAXI_TRIPS data\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set2a = valid_set2.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set2a[\"NUM_COLLISIONS_predicted\"] = linear_day_predictions_2.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set2a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq6Br408FteT",
        "outputId": "82300d16-b86d-44bc-89d6-16aca0edc3f0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c62348f9c60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.143507719039917\n",
            "\r\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c62348f9c60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "      day  dewp  visib  wdsp  mxpsd  gust  NUM_COLLISIONS  \\\n",
            "3271    1  58.2    8.8  13.8   20.0  27.0             641   \n",
            "3709    7  39.8    9.6  11.9   22.0  28.9             241   \n",
            "3135    7  47.9    6.0  15.3   19.0  28.0             256   \n",
            "4263    4  39.1    5.3  16.4   28.9  36.9             266   \n",
            "1746    4  35.1    9.8   6.4   15.0  22.9             635   \n",
            "...   ...   ...    ...   ...    ...   ...             ...   \n",
            "2771    2  59.7    8.7  14.1   18.1  22.9             318   \n",
            "2355    2  22.6   10.0  12.2   17.1  24.1             282   \n",
            "2506    5  44.3   10.0  12.1   17.1  22.9             777   \n",
            "3595    7  34.1    9.1  13.4   22.0  28.0             541   \n",
            "2922    3  38.5    5.0  12.3   19.0  25.1             565   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3271      [459.8818664550781]  \n",
            "3709     [500.35137939453125]  \n",
            "3135     [318.15301513671875]  \n",
            "4263      [367.7852478027344]  \n",
            "1746     [506.82220458984375]  \n",
            "...                       ...  \n",
            "2771     [445.93597412109375]  \n",
            "2355      [549.2774047851562]  \n",
            "2506     [501.24090576171875]  \n",
            "3595        [490.22998046875]  \n",
            "2922        [330.53076171875]  \n",
            "\n",
            "[137 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Mean Dewpoint"
      ],
      "metadata": {
        "id": "Ddfl5gClbFe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "#get the data\n",
        "df_input_data3 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers3 = [\"day\", \"temp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input3 = pd.concat(df_input_data3, axis=1, keys=df_input_headers3)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input3 = df_input3.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input3[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set3 = df_input3.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set3 = df_input3.drop(training_set3.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set3 = df_input3.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features3 = training_set2.copy()\n",
        "valid_features3 = valid_set3.copy()\n",
        "test_features3 = test_set3.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels3 = training_features3.pop('NUM_COLLISIONS')\n",
        "test_labels3 = test_features3.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features3.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels3 = training_labels3/SCALE_NUM_COLLISIONS\n",
        "test_labels3 = test_labels3/SCALE_NUM_COLLISIONS\n",
        "\n",
        "# use the training set training_features for the normalisation layer in order to try and fit to the output\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features3))\n",
        "\n",
        "# create model and add normaliser\n",
        "model_3 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# create a sequential model, with designated optimiser and loss parameter mean absolute error\n",
        "model_3.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaEJ3OgDbIK4",
        "outputId": "c8d0798a-b661-4697-ab35-993cb44c1b33"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  visib  wdsp  mxpsd  gust  NUM_COLLISIONS\n",
            "2807    7  62.2    8.8  12.3   18.1  22.9             206\n",
            "2288    3  59.8    9.6  12.3   15.9  21.0             655\n",
            "3995    2  48.3    3.4  17.3   25.1  35.0             608\n",
            "2333    1  37.8    9.9  13.3   17.1  21.0             238\n",
            "4127    4  37.7    8.2  19.9   27.0  42.9             252\n",
            "2973    1  68.3   10.0  11.2   19.0  22.0             521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use training features and labels to fit the model. Run 100 times and include 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_3.fit(\n",
        "    training_features3,\n",
        "    training_labels3,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id8zsmCsbIUn",
        "outputId": "1e23f3bc-3b29-4020-b253-dc5c8f8122d7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.3 s, sys: 648 ms, total: 13.9 s\n",
            "Wall time: 14.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model using the test features and labels. Obtain mean absolute error\n",
        "mean_absolute_error_model_3 = model_3.evaluate(\n",
        "    test_features3,\n",
        "    test_labels3, verbose=0)\n",
        "\n",
        "# Print the mean absolute error of the model\n",
        "print(mean_absolute_error_model_3)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "linear_day_predictions_3 = model_3.predict(valid_features3)*1200 # scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set3a = valid_set3.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set3a[\"NUM_COLLISIONS_predicted\"] = linear_day_predictions_3.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set3a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTn94PWwbIb1",
        "outputId": "279995ca-c9a5-46e6-9ad7-dd14b742d9e7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13664111495018005\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "      day  temp  visib  wdsp  mxpsd  gust  NUM_COLLISIONS  \\\n",
            "3271    1  62.3    8.8  13.8   20.0  27.0             641   \n",
            "3709    7  48.8    9.6  11.9   22.0  28.9             241   \n",
            "2115    2  50.8    9.9  10.6   15.9  18.1             312   \n",
            "3298    2  43.1   10.0  17.5   20.0  28.0             340   \n",
            "3679    5  28.0   10.0  13.1   22.0  29.9             702   \n",
            "...   ...   ...    ...   ...    ...   ...             ...   \n",
            "2789    5  44.8    2.9   8.8   18.1  26.0             582   \n",
            "3775    6  42.1   10.0  16.7   22.9  32.1             624   \n",
            "3512    6  43.8    5.0  11.8   21.0  31.1             455   \n",
            "3595    7  37.8    9.1  13.4   22.0  28.0             541   \n",
            "2686    3  53.9    9.0   9.2   18.1  26.0             641   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3271      [507.5648498535156]  \n",
            "3709       [517.657958984375]  \n",
            "2115      [527.4697875976562]  \n",
            "3298      [516.9403076171875]  \n",
            "3679      [560.1661987304688]  \n",
            "...                       ...  \n",
            "2789      [409.0769958496094]  \n",
            "3775      [513.2979736328125]  \n",
            "3512      [436.6128845214844]  \n",
            "3595      [525.0535888671875]  \n",
            "2686      [497.9320983886719]  \n",
            "\n",
            "[137 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Mean Visibility"
      ],
      "metadata": {
        "id": "HqJL1bo9dJwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "#get the data\n",
        "df_input_data4 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers4 = [\"day\", \"temp\", \"dewp\", \"wdsp\", \"mxpsd\", \"gust\", \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input4 = pd.concat(df_input_data4, axis=1, keys=df_input_headers4)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input4 = df_input4.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input4[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set4 = df_input4.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set4 = df_input4.drop(training_set4.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set4 = df_input4.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features4 = training_set4.copy()\n",
        "valid_features4 = valid_set4.copy()\n",
        "test_features4 = test_set4.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels4 = training_features4.pop('NUM_COLLISIONS')\n",
        "test_labels4 = test_features4.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features4.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels4 = training_labels4/SCALE_NUM_COLLISIONS\n",
        "test_labels4 = test_labels4/SCALE_NUM_COLLISIONS\n",
        "\n",
        "# use the training set training_features for the normalisation layer in order to try and fit to the output\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features4))\n",
        "\n",
        "# create model and add normaliser\n",
        "model_4 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# create a sequential model, with designated optimiser and loss parameter mean absolute error\n",
        "model_4.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFtJuJGVdKFQ",
        "outputId": "703ba999-c8b6-4b65-b29a-ec63c29829c7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  wdsp  mxpsd  gust  NUM_COLLISIONS\n",
            "2807    7  62.2  56.2  12.3   18.1  22.9             206\n",
            "2288    3  59.8  55.0  12.3   15.9  21.0             655\n",
            "3995    2  48.3  45.7  17.3   25.1  35.0             608\n",
            "2333    1  37.8  31.6  13.3   17.1  21.0             238\n",
            "4127    4  37.7  24.6  19.9   27.0  42.9             252\n",
            "2973    1  68.3  64.1  11.2   19.0  22.0             521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use training features and labels to fit the model. Run 100 times and include 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_4.fit(\n",
        "    training_features4,\n",
        "    training_labels4,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTYB5vkJdKQl",
        "outputId": "a4a77eb9-7da2-4f79-8350-64b3dfdf84da"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.2 s, sys: 640 ms, total: 13.8 s\n",
            "Wall time: 14.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model using the test features and labels. Obtain mean absolute error\n",
        "mean_absolute_error_model_4 = model_4.evaluate(\n",
        "    test_features4,\n",
        "    test_labels4, verbose=0)\n",
        "\n",
        "# Print the mean absolute error of the model\n",
        "print(mean_absolute_error_model_4)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "linear_day_predictions_4 = model_4.predict(valid_features4)*1200 # essentially 600000 in this instance would give back realistic numbers based on the TAXI_TRIPS data\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set4a = valid_set4.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set4a[\"NUM_COLLISIONS_predicted\"] = linear_day_predictions_4.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set4a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm_Wy6F3dKaI",
        "outputId": "1c72fa86-5732-49ab-b264-2425b1647636"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13224492967128754\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "      day  temp  dewp  wdsp  mxpsd  gust  NUM_COLLISIONS  \\\n",
            "3271    1  62.3  58.2  13.8   20.0  27.0             641   \n",
            "3709    7  48.8  39.8  11.9   22.0  28.9             241   \n",
            "3135    7  49.6  47.9  15.3   19.0  28.0             256   \n",
            "4263    4  43.7  39.1  16.4   28.9  36.9             266   \n",
            "1746    4  42.7  35.1   6.4   15.0  22.9             635   \n",
            "...   ...   ...   ...   ...    ...   ...             ...   \n",
            "2771    2  63.3  59.7  14.1   18.1  22.9             318   \n",
            "2355    2  37.1  22.6  12.2   17.1  24.1             282   \n",
            "2506    5  53.6  44.3  12.1   17.1  22.9             777   \n",
            "3595    7  37.8  34.1  13.4   22.0  28.0             541   \n",
            "2922    3  42.1  38.5  12.3   19.0  25.1             565   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3271      [594.3936157226562]  \n",
            "3709      [500.6744079589844]  \n",
            "3135     [423.32659912109375]  \n",
            "4263     [494.33111572265625]  \n",
            "1746      [619.2216186523438]  \n",
            "...                       ...  \n",
            "2771      [585.9802856445312]  \n",
            "2355      [611.2922973632812]  \n",
            "2506      [548.4043579101562]  \n",
            "3595              [478.96875]  \n",
            "2922      [576.2396850585938]  \n",
            "\n",
            "[137 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Mean Wind Speed"
      ],
      "metadata": {
        "id": "hOb6JnjgeLVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "#get the data\n",
        "df_input_data5 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers5 = [\"day\", \"temp\", \"dewp\", \"visib\", \"mxpsd\", \"gust\", \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input5 = pd.concat(df_input_data5, axis=1, keys=df_input_headers5)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input5 = df_input5.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input5[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set5 = df_input5.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set5 = df_input5.drop(training_set5.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set5 = df_input5.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features5 = training_set5.copy()\n",
        "valid_features5 = valid_set5.copy()\n",
        "test_features5 = test_set5.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels5 = training_features5.pop('NUM_COLLISIONS')\n",
        "test_labels5 = test_features5.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features5.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels5 = training_labels5/SCALE_NUM_COLLISIONS\n",
        "test_labels5 = test_labels5/SCALE_NUM_COLLISIONS\n",
        "\n",
        "# use the training set training_features for the normalisation layer in order to try and fit to the output\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features5))\n",
        "\n",
        "# create model and add normaliser\n",
        "model_5 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# create a sequential model, with designated optimiser and loss parameter mean absolute error\n",
        "model_5.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yO14giB6eLvo",
        "outputId": "745c3171-193b-4292-c421-6e7dd44f613e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  visib  mxpsd  gust  NUM_COLLISIONS\n",
            "2807    7  62.2  56.2    8.8   18.1  22.9             206\n",
            "2288    3  59.8  55.0    9.6   15.9  21.0             655\n",
            "3995    2  48.3  45.7    3.4   25.1  35.0             608\n",
            "2333    1  37.8  31.6    9.9   17.1  21.0             238\n",
            "4127    4  37.7  24.6    8.2   27.0  42.9             252\n",
            "2973    1  68.3  64.1   10.0   19.0  22.0             521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use training features and labels to fit the model. Run 100 times and include 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_5.fit(\n",
        "    training_features5,\n",
        "    training_labels5,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVCjBXTMeLfa",
        "outputId": "5ebe6862-2001-40c3-d4af-28950f5aae69"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.6 s, sys: 681 ms, total: 14.3 s\n",
            "Wall time: 16.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model using the test features and labels. Obtain mean absolute error\n",
        "mean_absolute_error_model_5 = model_5.evaluate(\n",
        "    test_features5,\n",
        "    test_labels5, verbose=0)\n",
        "\n",
        "# Print the mean absolute error of the model\n",
        "print(mean_absolute_error_model_5)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "linear_day_predictions_5 = model_5.predict(valid_features5)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set5a = valid_set5.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set5a[\"NUM_COLLISIONS_predicted\"] = linear_day_predictions_5.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set5a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP1Aira8eLng",
        "outputId": "8b43ea5d-4efa-44a2-c231-3f584abdb535"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13423854112625122\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "      day  temp  dewp  visib  mxpsd  gust  NUM_COLLISIONS  \\\n",
            "3271    1  62.3  58.2    8.8   20.0  27.0             641   \n",
            "3709    7  48.8  39.8    9.6   22.0  28.9             241   \n",
            "3135    7  49.6  47.9    6.0   19.0  28.0             256   \n",
            "4263    4  43.7  39.1    5.3   28.9  36.9             266   \n",
            "1746    4  42.7  35.1    9.8   15.0  22.9             635   \n",
            "...   ...   ...   ...    ...    ...   ...             ...   \n",
            "2771    2  63.3  59.7    8.7   18.1  22.9             318   \n",
            "2355    2  37.1  22.6   10.0   17.1  24.1             282   \n",
            "2506    5  53.6  44.3   10.0   17.1  22.9             777   \n",
            "3595    7  37.8  34.1    9.1   22.0  28.0             541   \n",
            "2922    3  42.1  38.5    5.0   19.0  25.1             565   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3271      [663.5039672851562]  \n",
            "3709      [451.9273376464844]  \n",
            "3135     [476.23614501953125]  \n",
            "4263      [551.1253051757812]  \n",
            "1746      [508.0480041503906]  \n",
            "...                       ...  \n",
            "2771       [673.430419921875]  \n",
            "2355      [570.9918823242188]  \n",
            "2506       [542.861572265625]  \n",
            "3595     [417.24688720703125]  \n",
            "2922      [624.1095581054688]  \n",
            "\n",
            "[137 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Maximum Sustained Wind Speed"
      ],
      "metadata": {
        "id": "gS__yNcyfN9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "#get the data\n",
        "df_input_data6 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"gust\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers6 = [\"day\", \"temp\", \"dewp\", \"visib\", \"wdsp\", \"gust\", \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input6 = pd.concat(df_input_data6, axis=1, keys=df_input_headers6)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input6 = df_input6.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input6[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set6 = df_input6.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set6 = df_input6.drop(training_set6.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set6 = df_input6.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features6 = training_set6.copy()\n",
        "valid_features6 = valid_set6.copy()\n",
        "test_features6 = test_set6.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels6 = training_features6.pop('NUM_COLLISIONS')\n",
        "test_labels6 = test_features6.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features6.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels6 = training_labels6/SCALE_NUM_COLLISIONS\n",
        "test_labels6 = test_labels6/SCALE_NUM_COLLISIONS\n",
        "\n",
        "# use the training set training_features for the normalisation layer in order to try and fit to the output\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features6))\n",
        "\n",
        "# create model and add normaliser\n",
        "model_6 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# create a sequential model, with designated optimiser and loss parameter mean absolute error\n",
        "model_6.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjJdz8IvfONa",
        "outputId": "d93d9ce9-7596-4eb3-975a-d0467294b7e8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  visib  wdsp  gust  NUM_COLLISIONS\n",
            "2807    7  62.2  56.2    8.8  12.3  22.9             206\n",
            "2288    3  59.8  55.0    9.6  12.3  21.0             655\n",
            "3995    2  48.3  45.7    3.4  17.3  35.0             608\n",
            "2333    1  37.8  31.6    9.9  13.3  21.0             238\n",
            "4127    4  37.7  24.6    8.2  19.9  42.9             252\n",
            "2973    1  68.3  64.1   10.0  11.2  22.0             521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use training features and labels to fit the model. Run 100 times and include 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_6.fit(\n",
        "    training_features6,\n",
        "    training_labels6,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qudvbr5MfOV8",
        "outputId": "8baddb56-cd57-4dcc-eb34-a113bf574c97"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.4 s, sys: 679 ms, total: 14 s\n",
            "Wall time: 15.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model using the test features and labels. Obtain mean absolute error\n",
        "mean_absolute_error_model_6 = model_6.evaluate(\n",
        "    test_features6,\n",
        "    test_labels6, verbose=0)\n",
        "\n",
        "# Print the mean absolute error of the model\n",
        "print(mean_absolute_error_model_6)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "linear_day_predictions_6 = model_6.predict(valid_features6)*1200 # essentially 600000 in this instance would give back realistic numbers based on the TAXI_TRIPS data\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set6a = valid_set6.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set6a[\"NUM_COLLISIONS_predicted\"] = linear_day_predictions_6.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set6a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuqHIzROfOed",
        "outputId": "9da83c46-152e-4502-f3bd-8f5f5831e65e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1345731019973755\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "      day  temp  dewp  visib  wdsp  gust  NUM_COLLISIONS  \\\n",
            "3271    1  62.3  58.2    8.8  13.8  27.0             641   \n",
            "3709    7  48.8  39.8    9.6  11.9  28.9             241   \n",
            "3135    7  49.6  47.9    6.0  15.3  28.0             256   \n",
            "4263    4  43.7  39.1    5.3  16.4  36.9             266   \n",
            "1746    4  42.7  35.1    9.8   6.4  22.9             635   \n",
            "...   ...   ...   ...    ...   ...   ...             ...   \n",
            "2771    2  63.3  59.7    8.7  14.1  22.9             318   \n",
            "2355    2  37.1  22.6   10.0  12.2  24.1             282   \n",
            "2506    5  53.6  44.3   10.0  12.1  22.9             777   \n",
            "3595    7  37.8  34.1    9.1  13.4  28.0             541   \n",
            "2922    3  42.1  38.5    5.0  12.3  25.1             565   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3271      [692.6053466796875]  \n",
            "3709      [440.5789489746094]  \n",
            "3135      [401.3814697265625]  \n",
            "4263     [481.65582275390625]  \n",
            "1746      [495.0378723144531]  \n",
            "...                       ...  \n",
            "2771      [662.3707885742188]  \n",
            "2355      [579.9873657226562]  \n",
            "2506      [537.4639892578125]  \n",
            "3595      [373.4427795410156]  \n",
            "2922      [491.3050231933594]  \n",
            "\n",
            "[137 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Maximum Wind Gust"
      ],
      "metadata": {
        "id": "hryjoj86gLoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "#get the data\n",
        "df_input_data7 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers7 = [\"day\", \"temp\", \"dewp\", \"visib\", \"wdsp\", \"mxpsd\", \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input7 = pd.concat(df_input_data7, axis=1, keys=df_input_headers7)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input7 = df_input7.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input7[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set7 = df_input7.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set7 = df_input7.drop(training_set7.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set7 = df_input7.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features7 = training_set7.copy()\n",
        "valid_features7 = valid_set7.copy()\n",
        "test_features7 = test_set7.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels7 = training_features7.pop('NUM_COLLISIONS')\n",
        "test_labels7 = test_features7.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features7.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels7 = training_labels7/SCALE_NUM_COLLISIONS\n",
        "test_labels7 = test_labels7/SCALE_NUM_COLLISIONS\n",
        "\n",
        "# use the training set training_features for the normalisation layer in order to try and fit to the output\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features7))\n",
        "\n",
        "# create model and add normaliser\n",
        "model_7 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# create a sequential model, with designated optimiser and loss parameter mean absolute error\n",
        "model_7.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBn_crM7gLzf",
        "outputId": "5cc7eae3-4a72-47d1-8158-49fb9e723c6f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  visib  wdsp  mxpsd  NUM_COLLISIONS\n",
            "1785    3  62.9  51.9   10.0   8.2   15.0             686\n",
            "2807    7  62.2  56.2    8.8  12.3   18.1             206\n",
            "2476    5  66.9  63.6    7.8   8.7   17.1             752\n",
            "2288    3  59.8  55.0    9.6  12.3   15.9             655\n",
            "1697    2  55.3  48.6   10.0   9.6   15.0             720\n",
            "640     1  32.7  23.9    9.7   4.6   11.1             706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use training features and labels to fit the model. Run 100 times and include 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_7.fit(\n",
        "    training_features7,\n",
        "    training_labels7,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiBfllKIgL7E",
        "outputId": "44e353d9-8607-419f-d978-45e675721865"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 18.7 s, sys: 1 s, total: 19.7 s\n",
            "Wall time: 22.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model using the test features and labels. Obtain mean absolute error\n",
        "mean_absolute_error_model_7 = model_7.evaluate(\n",
        "    test_features7,\n",
        "    test_labels7, verbose=0)\n",
        "\n",
        "# Print the mean absolute error of the model\n",
        "print(mean_absolute_error_model_7)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "linear_day_predictions_7 = model_7.predict(valid_features7)*1200 # essentially 600000 in this instance would give back realistic numbers based on the TAXI_TRIPS data\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set7a = valid_set7.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set7a[\"NUM_COLLISIONS_predicted\"] = linear_day_predictions_7.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set7a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e0xHlmXgMCn",
        "outputId": "972437e0-2499-499e-dd52-f6c18e372c37"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1355268508195877\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "      day  temp  dewp  visib  wdsp  mxpsd  NUM_COLLISIONS  \\\n",
            "3727    4  24.9   4.8   10.0  19.2   22.9             412   \n",
            "3834    7  51.9  51.2    2.1  15.2   22.9             505   \n",
            "2859    6  44.9  36.6    9.4  12.2   18.1             549   \n",
            "3822    2  32.6  25.6    8.4  16.3   22.9             613   \n",
            "3578    2  60.5  55.1    8.7  14.4   21.0             715   \n",
            "...   ...   ...   ...    ...   ...    ...             ...   \n",
            "3537    4  46.6  41.2   10.0  12.7   21.0             197   \n",
            "2914    4  29.9  11.8   10.0  12.2   19.0             625   \n",
            "2765    5  61.7  58.6    4.0  14.4   18.1             708   \n",
            "2248    7  71.5  70.6    7.0  11.9   15.9             284   \n",
            "2010    3  41.1  38.4    6.5   6.0   15.9             583   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3727     [410.91107177734375]  \n",
            "3834     [240.20681762695312]  \n",
            "2859      [457.9117736816406]  \n",
            "3822       [398.500732421875]  \n",
            "3578     [454.98089599609375]  \n",
            "...                       ...  \n",
            "3537      [459.8057861328125]  \n",
            "2914      [512.1777954101562]  \n",
            "2765      [350.1043701171875]  \n",
            "2248      [416.9057312011719]  \n",
            "2010      [507.0890808105469]  \n",
            "\n",
            "[221 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Day of Week"
      ],
      "metadata": {
        "id": "q1CuGXf9hFE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "#get the data\n",
        "df_input_data8 = [df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers8 = [\"temp\", \"dewp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input8 = pd.concat(df_input_data8, axis=1, keys=df_input_headers8)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input8 = df_input8.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input8[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set8 = df_input8.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set8 = df_input8.drop(training_set8.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set8 = df_input8.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets.\n",
        "training_features8 = training_set8.copy()\n",
        "valid_features8 = valid_set8.copy()\n",
        "test_features8 = test_set8.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels8 = training_features8.pop('NUM_COLLISIONS')\n",
        "test_labels8 = test_features8.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features8.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels8 = training_labels8/SCALE_NUM_COLLISIONS\n",
        "test_labels8 = test_labels8/SCALE_NUM_COLLISIONS\n",
        "\n",
        "# use the training set training_features for the normalisation layer in order to try and fit to the output\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features8))\n",
        "\n",
        "# create model and add normaliser\n",
        "model_8 = tf.keras.Sequential([\n",
        "    normaliser,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# create a sequential model, with designated optimiser and loss parameter mean absolute error\n",
        "model_8.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grvaq7JphFQy",
        "outputId": "61cdb0cd-7900-4434-85d4-aa47aa415d13"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      temp  dewp  visib  wdsp  mxpsd  gust  NUM_COLLISIONS\n",
            "2807  62.2  56.2    8.8  12.3   18.1  22.9             206\n",
            "2288  59.8  55.0    9.6  12.3   15.9  21.0             655\n",
            "3995  48.3  45.7    3.4  17.3   25.1  35.0             608\n",
            "2333  37.8  31.6    9.9  13.3   17.1  21.0             238\n",
            "4127  37.7  24.6    8.2  19.9   27.0  42.9             252\n",
            "2973  68.3  64.1   10.0  11.2   19.0  22.0             521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use training features and labels to fit the model. Run 100 times and include 20% validation split.\n",
        "\n",
        "%%time\n",
        "history = model_8.fit(\n",
        "    training_features8,\n",
        "    training_labels8,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    validation_split = 0.2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuUGQ-7AhFZC",
        "outputId": "09d199b7-e2c8-45a5-a847-c2fcd602d3f3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.4 s, sys: 692 ms, total: 14.1 s\n",
            "Wall time: 14.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model using the test features and labels. Obtain mean absolute error\n",
        "mean_absolute_error_model_8 = model_8.evaluate(\n",
        "    test_features8,\n",
        "    test_labels8, verbose=0)\n",
        "\n",
        "# Print the mean absolute error of the model\n",
        "print(mean_absolute_error_model_8)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "linear_day_predictions_8 = model_8.predict(valid_features8)*1200 # essentially 600000 in this instance would give back realistic numbers based on the TAXI_TRIPS data\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set8a = valid_set8.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set8a[\"NUM_COLLISIONS_predicted\"] = linear_day_predictions_8.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set8a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VuIkozEhFhh",
        "outputId": "5f33a10c-b945-49d3-dfee-f62da5a06b89"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14146864414215088\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "      temp  dewp  visib  wdsp  mxpsd  gust  NUM_COLLISIONS  \\\n",
            "3271  62.3  58.2    8.8  13.8   20.0  27.0             641   \n",
            "3709  48.8  39.8    9.6  11.9   22.0  28.9             241   \n",
            "3135  49.6  47.9    6.0  15.3   19.0  28.0             256   \n",
            "4263  43.7  39.1    5.3  16.4   28.9  36.9             266   \n",
            "1746  42.7  35.1    9.8   6.4   15.0  22.9             635   \n",
            "...    ...   ...    ...   ...    ...   ...             ...   \n",
            "2771  63.3  59.7    8.7  14.1   18.1  22.9             318   \n",
            "2355  37.1  22.6   10.0  12.2   17.1  24.1             282   \n",
            "2506  53.6  44.3   10.0  12.1   17.1  22.9             777   \n",
            "3595  37.8  34.1    9.1  13.4   22.0  28.0             541   \n",
            "2922  42.1  38.5    5.0  12.3   19.0  25.1             565   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3271      [560.8511352539062]  \n",
            "3709      [535.4234008789062]  \n",
            "3135      [455.9004211425781]  \n",
            "4263      [405.0241394042969]  \n",
            "1746      [548.4365844726562]  \n",
            "...                       ...  \n",
            "2771          [603.064453125]  \n",
            "2355      [533.2682495117188]  \n",
            "2506         [593.3056640625]  \n",
            "3595      [490.8773193359375]  \n",
            "2922     [481.39459228515625]  \n",
            "\n",
            "[137 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare Mean Absolute Error"
      ],
      "metadata": {
        "id": "Gqq62UgR_wml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean absolute error results\")\n",
        "print(\"All variables: \" + str(mean_absolute_error_model_0))\n",
        "print(\"Significant variables: \" + str(mean_absolute_error_model_1))\n",
        "print(\"Significant variables except mean temperature: \" + str(mean_absolute_error_model_2))\n",
        "print(\"Significant variables except mean dewpoint: \" + str(mean_absolute_error_model_3))\n",
        "print(\"Significant variables except mean visibility: \" + str(mean_absolute_error_model_4))\n",
        "print(\"Significant variables except mean wind speed: \" + str(mean_absolute_error_model_5))\n",
        "print(\"Significant variables except maximum sustained wind speed: \" + str(mean_absolute_error_model_6))\n",
        "print(\"Significant variables except maximum wind gust: \" + str(mean_absolute_error_model_7))\n",
        "print(\"Significant variables except day of week: \" + str(mean_absolute_error_model_8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL32dcOLiHOC",
        "outputId": "b7206188-f97d-4c49-a412-2b64f0ea6719"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean absolute error results\n",
            "All variables: 0.1782083511352539\n",
            "Significant variables: 0.1279795616865158\n",
            "Significant variables except mean temperature: 0.143507719039917\n",
            "Significant variables except mean dewpoint: 0.13664111495018005\n",
            "Significant variables except mean visibility: 0.13224492967128754\n",
            "Significant variables except mean wind speed: 0.13423854112625122\n",
            "Significant variables except maximum sustained wind speed: 0.1345731019973755\n",
            "Significant variables except maximum wind gust: 0.1355268508195877\n",
            "Significant variables except day of week: 0.14146864414215088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DNN Modelling\n",
        "###All Variables"
      ],
      "metadata": {
        "id": "GnFZzjj3ATSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data9 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers9 = [\"day\", \"temp\", \"dewp\", \"slp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"max\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input9 = pd.concat(df_input_data9, axis=1, keys=df_input_headers9)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input9 = df_input9.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input9[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set9 = df_input9.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set9 = df_input9.drop(training_set9.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set9 = df_input9.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features9 = training_set9.copy()\n",
        "valid_features9 = valid_set9.copy()\n",
        "test_features9 = test_set9.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels9 = training_features9.pop('NUM_COLLISIONS')\n",
        "test_labels9 = test_features9.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features9.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels9 = training_labels9/SCALE_NUM_COLLISIONS\n",
        "test_labels9 = test_labels9/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features9))\n",
        "\n",
        "# Normalisation layer (12 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_1 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_1.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py7GHPCS6w-y",
        "outputId": "68fafbb4-8200-47cc-b60a-6fc64be9ba17"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  \\\n",
            "2807    7  62.2  56.2  1007.9    8.8  12.3   18.1  22.9  66.0  57.9  0.01   \n",
            "2288    3  59.8  55.0  1014.3    9.6  12.3   15.9  21.0  66.0  51.1  0.00   \n",
            "3995    2  48.3  45.7  1010.6    3.4  17.3   25.1  35.0  54.0  41.0  0.57   \n",
            "2333    1  37.8  31.6  1018.6    9.9  13.3   17.1  21.0  46.0  25.0  0.00   \n",
            "4127    4  37.7  24.6  1017.9    8.2  19.9   27.0  42.9  51.1  30.0  0.24   \n",
            "2973    1  68.3  64.1  1020.1   10.0  11.2   19.0  22.0  75.0  50.0  0.00   \n",
            "\n",
            "      fog  NUM_COLLISIONS  \n",
            "2807    1             206  \n",
            "2288    0             655  \n",
            "3995    1             608  \n",
            "2333    0             238  \n",
            "4127    0             252  \n",
            "2973    0             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_1.fit(\n",
        "    training_features9,\n",
        "    training_labels9,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbSFo8-DhEVC",
        "outputId": "940e4a6c-ebe1-4c3f-d2a6-ad1ffa278c92"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16 s, sys: 818 ms, total: 16.8 s\n",
            "Wall time: 16.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_1_results = dnn_model_1.evaluate(test_features9, test_labels9, verbose=0)\n",
        "print(dnn_model_1_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_1 = dnn_model_1.predict(valid_features9)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set9a = valid_set9.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set9a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_1.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set9a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3X1qbkZhKcm",
        "outputId": "605a8a61-a83c-43a7-bfb6-4e1f12ec314a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14019443094730377\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  \\\n",
            "1504    6  41.2  37.6  1017.1    9.1   7.5   14.0  20.0  48.9  37.9  0.10   \n",
            "2408    7  69.4  59.9  1014.4    9.5  11.8   17.1  22.9  75.0  64.0  0.00   \n",
            "3567    7  39.4  34.8  1021.0    8.3  12.4   21.0  29.9  45.0  33.1  0.55   \n",
            "1720    4  61.0  61.0  1005.3    4.2   8.6   15.0  22.0  66.9  54.0  0.03   \n",
            "2977    6  61.4  58.0  1019.9    6.5  11.1   19.0  28.0  66.9  52.0  0.08   \n",
            "...   ...   ...   ...     ...    ...   ...    ...   ...   ...   ...   ...   \n",
            "4357    4  38.9  36.3  1010.4    4.0  22.0   34.0  45.1  46.9  26.1  0.20   \n",
            "3720    7  56.5  48.2  1026.6   10.0  15.5   22.0  25.1  63.0  51.1  0.00   \n",
            "883     7  63.8  50.3  1015.4    9.0   6.4   12.0  15.9  72.0  55.4  0.22   \n",
            "1273    6  75.7  65.2  1023.6    7.3   3.2   13.0  18.1  84.0  62.6  0.01   \n",
            "3173    4  30.9  10.8  1015.8   10.0  14.5   20.0  27.0  37.0  25.0  0.00   \n",
            "\n",
            "      fog  NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504    0             181      [451.1587219238281]  \n",
            "2408    0             519     [471.48309326171875]  \n",
            "3567    0             472     [404.05255126953125]  \n",
            "1720    1             279      [628.2706298828125]  \n",
            "2977    0             328      [490.7253723144531]  \n",
            "...   ...             ...                      ...  \n",
            "4357    0             233     [241.82388305664062]  \n",
            "3720    0             432      [535.5169067382812]  \n",
            "883     0             480         [544.7041015625]  \n",
            "1273    1             474        [594.06591796875]  \n",
            "3173    0             666      [618.6632690429688]  \n",
            "\n",
            "[136 rows x 14 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Mean Temperature"
      ],
      "metadata": {
        "id": "WGl0Mx_ki_Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data10 = [df_shuffle[\"day\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers10 = [\"day\", \"dewp\", \"slp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"max\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input10 = pd.concat(df_input_data10, axis=1, keys=df_input_headers10)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input10 = df_input10.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input10[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set10 = df_input10.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set10 = df_input10.drop(training_set10.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set10 = df_input10.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features10 = training_set10.copy()\n",
        "valid_features10 = valid_set10.copy()\n",
        "test_features10 = test_set10.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels10 = training_features10.pop('NUM_COLLISIONS')\n",
        "test_labels10 = test_features10.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features10.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels10 = training_labels10/SCALE_NUM_COLLISIONS\n",
        "test_labels10 = test_labels10/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features10))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_2 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_2.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_19ZpLrk33w",
        "outputId": "119d6c2c-7e53-4d2e-cea0-462ee8725d51"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  dewp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  fog  \\\n",
            "2807    7  56.2  1007.9    8.8  12.3   18.1  22.9  66.0  57.9  0.01    1   \n",
            "2288    3  55.0  1014.3    9.6  12.3   15.9  21.0  66.0  51.1  0.00    0   \n",
            "3995    2  45.7  1010.6    3.4  17.3   25.1  35.0  54.0  41.0  0.57    1   \n",
            "2333    1  31.6  1018.6    9.9  13.3   17.1  21.0  46.0  25.0  0.00    0   \n",
            "4127    4  24.6  1017.9    8.2  19.9   27.0  42.9  51.1  30.0  0.24    0   \n",
            "2973    1  64.1  1020.1   10.0  11.2   19.0  22.0  75.0  50.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_2.fit(\n",
        "    training_features10,\n",
        "    training_labels10,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBBGKAQIk4EJ",
        "outputId": "400f5e4d-4e68-4288-a4bc-2add3bfcd112"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.3 s, sys: 757 ms, total: 17.1 s\n",
            "Wall time: 17.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_2_results = dnn_model_2.evaluate(test_features10, test_labels10, verbose=0)\n",
        "print(dnn_model_2_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_2 = dnn_model_2.predict(valid_features10)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set10a = valid_set10.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set10a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_2.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set10a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojFKSkGFk4Hj",
        "outputId": "69b45e4f-a946-4c43-a612-a56c1d8ace18"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1428210735321045\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "      day  dewp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  fog  \\\n",
            "1504    6  37.6  1017.1    9.1   7.5   14.0  20.0  48.9  37.9  0.10    0   \n",
            "2408    7  59.9  1014.4    9.5  11.8   17.1  22.9  75.0  64.0  0.00    0   \n",
            "3567    7  34.8  1021.0    8.3  12.4   21.0  29.9  45.0  33.1  0.55    0   \n",
            "1720    4  61.0  1005.3    4.2   8.6   15.0  22.0  66.9  54.0  0.03    1   \n",
            "2977    6  58.0  1019.9    6.5  11.1   19.0  28.0  66.9  52.0  0.08    0   \n",
            "...   ...   ...     ...    ...   ...    ...   ...   ...   ...   ...  ...   \n",
            "4357    4  36.3  1010.4    4.0  22.0   34.0  45.1  46.9  26.1  0.20    0   \n",
            "3720    7  48.2  1026.6   10.0  15.5   22.0  25.1  63.0  51.1  0.00    0   \n",
            "883     7  50.3  1015.4    9.0   6.4   12.0  15.9  72.0  55.4  0.22    0   \n",
            "1273    6  65.2  1023.6    7.3   3.2   13.0  18.1  84.0  62.6  0.01    1   \n",
            "3173    4  10.8  1015.8   10.0  14.5   20.0  27.0  37.0  25.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504             181     [361.66949462890625]  \n",
            "2408             519      [516.8641967773438]  \n",
            "3567             472      [522.7064819335938]  \n",
            "1720             279       [396.063232421875]  \n",
            "2977             328      [260.8016357421875]  \n",
            "...              ...                      ...  \n",
            "4357             233     [230.88784790039062]  \n",
            "3720             432     [467.67828369140625]  \n",
            "883              480     [447.72796630859375]  \n",
            "1273             474      [337.3829345703125]  \n",
            "3173             666      [634.6002197265625]  \n",
            "\n",
            "[136 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Mean Dewpoint\n"
      ],
      "metadata": {
        "id": "7Z-SvdM0i_VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data11 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers11 = [\"day\", \"temp\", \"slp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"max\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input11 = pd.concat(df_input_data11, axis=1, keys=df_input_headers11)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input11 = df_input11.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input11[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set11 = df_input11.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set11 = df_input11.drop(training_set11.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set11 = df_input11.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features11 = training_set11.copy()\n",
        "valid_features11 = valid_set11.copy()\n",
        "test_features11 = test_set11.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels11 = training_features11.pop('NUM_COLLISIONS')\n",
        "test_labels11 = test_features11.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features11.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels11 = training_labels11/SCALE_NUM_COLLISIONS\n",
        "test_labels11 = test_labels11/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features11))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_3 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_3.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTsHhlUCk4nn",
        "outputId": "45442173-df65-4e3e-bf72-5c8e70d206b7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  fog  \\\n",
            "2807    7  62.2  1007.9    8.8  12.3   18.1  22.9  66.0  57.9  0.01    1   \n",
            "2288    3  59.8  1014.3    9.6  12.3   15.9  21.0  66.0  51.1  0.00    0   \n",
            "3995    2  48.3  1010.6    3.4  17.3   25.1  35.0  54.0  41.0  0.57    1   \n",
            "2333    1  37.8  1018.6    9.9  13.3   17.1  21.0  46.0  25.0  0.00    0   \n",
            "4127    4  37.7  1017.9    8.2  19.9   27.0  42.9  51.1  30.0  0.24    0   \n",
            "2973    1  68.3  1020.1   10.0  11.2   19.0  22.0  75.0  50.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_3.fit(\n",
        "    training_features11,\n",
        "    training_labels11,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUus0qCJk4th",
        "outputId": "ce9faae8-e899-49dc-ffcc-a7c227121099"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.9 s, sys: 745 ms, total: 16.6 s\n",
            "Wall time: 16.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_3_results = dnn_model_3.evaluate(test_features11, test_labels11, verbose=0)\n",
        "print(dnn_model_3_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_3 = dnn_model_3.predict(valid_features11)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set11a = valid_set11.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set11a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_3.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set11a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kebxDBYLk40S",
        "outputId": "a5830c41-c043-44d9-f56a-c567dd7d2e4b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1623493731021881\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "      day  temp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  fog  \\\n",
            "3402    5  42.4  1026.5   10.0  13.4   21.0  28.9  50.0  28.0  0.00    0   \n",
            "1643    3  58.8  1026.2   10.0   8.9   14.0  22.0  64.9  51.1  0.00    0   \n",
            "4205    2  43.3  1014.5    6.6  16.0   28.0  39.0  48.0  28.9  0.35    0   \n",
            "3736    5  38.4  1015.4    9.9  10.7   22.9  32.1  52.0  27.0  0.01    0   \n",
            "3321    3  38.1  1018.9   10.0  14.7   20.0  27.0  46.0  21.9  0.00    0   \n",
            "...   ...   ...     ...    ...   ...    ...   ...   ...   ...   ...  ...   \n",
            "4357    4  38.9  1010.4    4.0  22.0   34.0  45.1  46.9  26.1  0.20    0   \n",
            "3483    7  51.3  1017.2   10.0  14.2   21.0  28.0  57.0  37.9  0.00    0   \n",
            "883     7  63.8  1015.4    9.0   6.4   12.0  15.9  72.0  55.4  0.22    0   \n",
            "1273    6  75.7  1023.6    7.3   3.2   13.0  18.1  84.0  62.6  0.01    1   \n",
            "2071    5  70.8  1008.1    7.9  10.9   15.9  21.0  80.1  64.0  0.18    0   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "3402             309     [318.71270751953125]  \n",
            "1643             268      [345.2427062988281]  \n",
            "4205             201     [247.25372314453125]  \n",
            "3736             235     [485.75054931640625]  \n",
            "3321             285     [203.59568786621094]  \n",
            "...              ...                      ...  \n",
            "4357             233     [208.24545288085938]  \n",
            "3483             518      [449.8907470703125]  \n",
            "883              480     [424.82525634765625]  \n",
            "1273             474      [427.2977600097656]  \n",
            "2071             747      [420.5141296386719]  \n",
            "\n",
            "[136 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Mean Sea Level Pressure"
      ],
      "metadata": {
        "id": "q8kLVuIUi_iV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data12 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers12 = [\"day\", \"temp\", \"dewp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"max\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input12 = pd.concat(df_input_data12, axis=1, keys=df_input_headers12)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input12 = df_input12.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input12[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set12 = df_input12.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set12 = df_input12.drop(training_set12.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set12 = df_input12.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features12 = training_set12.copy()\n",
        "valid_features12 = valid_set12.copy()\n",
        "test_features12 = test_set12.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels12 = training_features12.pop('NUM_COLLISIONS')\n",
        "test_labels12 = test_features12.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features12.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels12 = training_labels12/SCALE_NUM_COLLISIONS\n",
        "test_labels12 = test_labels12/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features12))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_4 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_4.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WqJSlrGk5iG",
        "outputId": "e3dd947a-39a9-4d6e-e9b9-75716f788fdc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  visib  wdsp  mxpsd  gust   max   min  prcp  fog  \\\n",
            "2807    7  62.2  56.2    8.8  12.3   18.1  22.9  66.0  57.9  0.01    1   \n",
            "2288    3  59.8  55.0    9.6  12.3   15.9  21.0  66.0  51.1  0.00    0   \n",
            "3995    2  48.3  45.7    3.4  17.3   25.1  35.0  54.0  41.0  0.57    1   \n",
            "2333    1  37.8  31.6    9.9  13.3   17.1  21.0  46.0  25.0  0.00    0   \n",
            "4127    4  37.7  24.6    8.2  19.9   27.0  42.9  51.1  30.0  0.24    0   \n",
            "2973    1  68.3  64.1   10.0  11.2   19.0  22.0  75.0  50.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_4.fit(\n",
        "    training_features12,\n",
        "    training_labels12,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl6RRAoLk5lA",
        "outputId": "18e77576-8906-4805-ca26-6dc8808f7d27"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.8 s, sys: 766 ms, total: 16.6 s\n",
            "Wall time: 16.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_4_results = dnn_model_4.evaluate(test_features12, test_labels12, verbose=0)\n",
        "print(dnn_model_4_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_4 = dnn_model_4.predict(valid_features12)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set12a = valid_set12.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set12a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_4.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set12a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQfcP0Ohk5oK",
        "outputId": "9d1bcf5c-f6c2-413b-a06f-8ac6729fbb97"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1399620622396469\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "      day  temp  dewp  visib  wdsp  mxpsd  gust   max   min  prcp  fog  \\\n",
            "1504    6  41.2  37.6    9.1   7.5   14.0  20.0  48.9  37.9  0.10    0   \n",
            "2408    7  69.4  59.9    9.5  11.8   17.1  22.9  75.0  64.0  0.00    0   \n",
            "3567    7  39.4  34.8    8.3  12.4   21.0  29.9  45.0  33.1  0.55    0   \n",
            "1720    4  61.0  61.0    4.2   8.6   15.0  22.0  66.9  54.0  0.03    1   \n",
            "2977    6  61.4  58.0    6.5  11.1   19.0  28.0  66.9  52.0  0.08    0   \n",
            "...   ...   ...   ...    ...   ...    ...   ...   ...   ...   ...  ...   \n",
            "4357    4  38.9  36.3    4.0  22.0   34.0  45.1  46.9  26.1  0.20    0   \n",
            "3720    7  56.5  48.2   10.0  15.5   22.0  25.1  63.0  51.1  0.00    0   \n",
            "883     7  63.8  50.3    9.0   6.4   12.0  15.9  72.0  55.4  0.22    0   \n",
            "1273    6  75.7  65.2    7.3   3.2   13.0  18.1  84.0  62.6  0.01    1   \n",
            "3173    4  30.9  10.8   10.0  14.5   20.0  27.0  37.0  25.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504             181      [223.8363800048828]  \n",
            "2408             519      [622.8245849609375]  \n",
            "3567             472       [385.449951171875]  \n",
            "1720             279      [442.7760009765625]  \n",
            "2977             328      [477.9197082519531]  \n",
            "...              ...                      ...  \n",
            "4357             233          [280.349609375]  \n",
            "3720             432      [504.4044189453125]  \n",
            "883              480     [510.66741943359375]  \n",
            "1273             474      [507.8678283691406]  \n",
            "3173             666      [578.7230224609375]  \n",
            "\n",
            "[136 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Maximum Temperature"
      ],
      "metadata": {
        "id": "URMaKGMPi_tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data13 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"],\n",
        "                   df_shuffle[\"gust\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"], df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers13 = [\"day\", \"temp\", \"dewp\", \"slp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input13 = pd.concat(df_input_data13, axis=1, keys=df_input_headers13)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input13 = df_input13.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input13[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set13 = df_input13.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set13 = df_input13.drop(training_set13.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set13 = df_input13.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features13 = training_set13.copy()\n",
        "valid_features13 = valid_set13.copy()\n",
        "test_features13 = test_set13.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels13 = training_features13.pop('NUM_COLLISIONS')\n",
        "test_labels13 = test_features13.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features13.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels13 = training_labels13/SCALE_NUM_COLLISIONS\n",
        "test_labels13 = test_labels13/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features13))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_5 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_5.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3kiEQ0Xk6RN",
        "outputId": "55e7e4d7-72a0-4e7e-a4c4-eac81b4f6bd6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   min  prcp  fog  \\\n",
            "2807    7  62.2  56.2  1007.9    8.8  12.3   18.1  22.9  57.9  0.01    1   \n",
            "2288    3  59.8  55.0  1014.3    9.6  12.3   15.9  21.0  51.1  0.00    0   \n",
            "3995    2  48.3  45.7  1010.6    3.4  17.3   25.1  35.0  41.0  0.57    1   \n",
            "2333    1  37.8  31.6  1018.6    9.9  13.3   17.1  21.0  25.0  0.00    0   \n",
            "4127    4  37.7  24.6  1017.9    8.2  19.9   27.0  42.9  30.0  0.24    0   \n",
            "2973    1  68.3  64.1  1020.1   10.0  11.2   19.0  22.0  50.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_5.fit(\n",
        "    training_features13,\n",
        "    training_labels13,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utH0ugb3k6UV",
        "outputId": "432e3f1d-7cc2-4e5a-d8ca-0ce470754d74"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17.2 s, sys: 788 ms, total: 17.9 s\n",
            "Wall time: 18.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_5_results = dnn_model_5.evaluate(test_features13, test_labels13, verbose=0)\n",
        "print(dnn_model_5_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_5 = dnn_model_5.predict(valid_features13)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set13a = valid_set13.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set13a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_5.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set13a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XITQDtqGk6Xd",
        "outputId": "80fd4498-daef-4df2-acc9-dd6634ab2912"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14132946729660034\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   min  prcp  fog  \\\n",
            "1504    6  41.2  37.6  1017.1    9.1   7.5   14.0  20.0  37.9  0.10    0   \n",
            "2408    7  69.4  59.9  1014.4    9.5  11.8   17.1  22.9  64.0  0.00    0   \n",
            "3567    7  39.4  34.8  1021.0    8.3  12.4   21.0  29.9  33.1  0.55    0   \n",
            "1720    4  61.0  61.0  1005.3    4.2   8.6   15.0  22.0  54.0  0.03    1   \n",
            "2977    6  61.4  58.0  1019.9    6.5  11.1   19.0  28.0  52.0  0.08    0   \n",
            "...   ...   ...   ...     ...    ...   ...    ...   ...   ...   ...  ...   \n",
            "4357    4  38.9  36.3  1010.4    4.0  22.0   34.0  45.1  26.1  0.20    0   \n",
            "3720    7  56.5  48.2  1026.6   10.0  15.5   22.0  25.1  51.1  0.00    0   \n",
            "883     7  63.8  50.3  1015.4    9.0   6.4   12.0  15.9  55.4  0.22    0   \n",
            "1273    6  75.7  65.2  1023.6    7.3   3.2   13.0  18.1  62.6  0.01    1   \n",
            "3173    4  30.9  10.8  1015.8   10.0  14.5   20.0  27.0  25.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504             181      [459.9565734863281]  \n",
            "2408             519      [658.5000610351562]  \n",
            "3567             472     [473.98529052734375]  \n",
            "1720             279      [467.9998779296875]  \n",
            "2977             328      [438.3970031738281]  \n",
            "...              ...                      ...  \n",
            "4357             233     [243.24356079101562]  \n",
            "3720             432         [570.2724609375]  \n",
            "883              480      [611.3917236328125]  \n",
            "1273             474       [550.914794921875]  \n",
            "3173             666      [631.0171508789062]  \n",
            "\n",
            "[136 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Minimum Temperature"
      ],
      "metadata": {
        "id": "O-kW_-Nri_1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data14 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"],\n",
        "                   df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"], df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers14 = [\"day\", \"temp\", \"dewp\", \"slp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"max\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input14 = pd.concat(df_input_data14, axis=1, keys=df_input_headers14)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input14 = df_input14.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input14[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set14 = df_input14.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set14 = df_input14.drop(training_set14.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set14 = df_input14.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features14 = training_set14.copy()\n",
        "valid_features14 = valid_set14.copy()\n",
        "test_features14 = test_set14.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels14 = training_features14.pop('NUM_COLLISIONS')\n",
        "test_labels14 = test_features14.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features14.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels14 = training_labels14/SCALE_NUM_COLLISIONS\n",
        "test_labels14 = test_labels14/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features14))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_6 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_6.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqQRSGC6k7Fj",
        "outputId": "e8aa05fe-3ebd-47ff-ab71-00106d6886e8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   max  prcp  fog  \\\n",
            "2807    7  62.2  56.2  1007.9    8.8  12.3   18.1  22.9  66.0  0.01    1   \n",
            "2288    3  59.8  55.0  1014.3    9.6  12.3   15.9  21.0  66.0  0.00    0   \n",
            "3995    2  48.3  45.7  1010.6    3.4  17.3   25.1  35.0  54.0  0.57    1   \n",
            "2333    1  37.8  31.6  1018.6    9.9  13.3   17.1  21.0  46.0  0.00    0   \n",
            "4127    4  37.7  24.6  1017.9    8.2  19.9   27.0  42.9  51.1  0.24    0   \n",
            "2973    1  68.3  64.1  1020.1   10.0  11.2   19.0  22.0  75.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_6.fit(\n",
        "    training_features14,\n",
        "    training_labels14,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWMhc0zZk7It",
        "outputId": "78f9b2b8-3a21-4668-fd63-9572f8874f3c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.1 s, sys: 789 ms, total: 16.9 s\n",
            "Wall time: 16 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_6_results = dnn_model_6.evaluate(test_features14, test_labels14, verbose=0)\n",
        "print(dnn_model_6_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_6 = dnn_model_6.predict(valid_features14)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set14a = valid_set14.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set14a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_6.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set14a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO92eqiZk7Ll",
        "outputId": "1f25e057-f65a-48e5-b084-acd6f1f24717"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13199076056480408\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   max  prcp  fog  \\\n",
            "1504    6  41.2  37.6  1017.1    9.1   7.5   14.0  20.0  48.9  0.10    0   \n",
            "2408    7  69.4  59.9  1014.4    9.5  11.8   17.1  22.9  75.0  0.00    0   \n",
            "3567    7  39.4  34.8  1021.0    8.3  12.4   21.0  29.9  45.0  0.55    0   \n",
            "1720    4  61.0  61.0  1005.3    4.2   8.6   15.0  22.0  66.9  0.03    1   \n",
            "2977    6  61.4  58.0  1019.9    6.5  11.1   19.0  28.0  66.9  0.08    0   \n",
            "...   ...   ...   ...     ...    ...   ...    ...   ...   ...   ...  ...   \n",
            "4357    4  38.9  36.3  1010.4    4.0  22.0   34.0  45.1  46.9  0.20    0   \n",
            "3720    7  56.5  48.2  1026.6   10.0  15.5   22.0  25.1  63.0  0.00    0   \n",
            "883     7  63.8  50.3  1015.4    9.0   6.4   12.0  15.9  72.0  0.22    0   \n",
            "1273    6  75.7  65.2  1023.6    7.3   3.2   13.0  18.1  84.0  0.01    1   \n",
            "3173    4  30.9  10.8  1015.8   10.0  14.5   20.0  27.0  37.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504             181      [400.1181335449219]  \n",
            "2408             519      [521.6683349609375]  \n",
            "3567             472     [387.57977294921875]  \n",
            "1720             279     [426.16766357421875]  \n",
            "2977             328     [450.74737548828125]  \n",
            "...              ...                      ...  \n",
            "4357             233     [218.29710388183594]  \n",
            "3720             432      [443.8787841796875]  \n",
            "883              480     [398.03826904296875]  \n",
            "1273             474     [473.48541259765625]  \n",
            "3173             666       [658.265869140625]  \n",
            "\n",
            "[136 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Reported Fog"
      ],
      "metadata": {
        "id": "LdoWxrRAi_9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data15 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"],\n",
        "                   df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers15 = [\"day\", \"temp\", \"dewp\", \"slp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"max\", \"min\", \"prcp\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input15 = pd.concat(df_input_data15, axis=1, keys=df_input_headers15)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input15 = df_input15.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input15[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set15 = df_input15.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set15 = df_input15.drop(training_set15.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set15 = df_input15.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features15 = training_set15.copy()\n",
        "valid_features15 = valid_set15.copy()\n",
        "test_features15 = test_set15.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels15 = training_features15.pop('NUM_COLLISIONS')\n",
        "test_labels15 = test_features15.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features15.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels15 = training_labels15/SCALE_NUM_COLLISIONS\n",
        "test_labels15 = test_labels15/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features15))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_7 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_7.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0QB9Viak71x",
        "outputId": "ffbac93e-70a2-41fa-d1bf-70836e8d6536"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  \\\n",
            "2807    7  62.2  56.2  1007.9    8.8  12.3   18.1  22.9  66.0  57.9  0.01   \n",
            "2288    3  59.8  55.0  1014.3    9.6  12.3   15.9  21.0  66.0  51.1  0.00   \n",
            "3995    2  48.3  45.7  1010.6    3.4  17.3   25.1  35.0  54.0  41.0  0.57   \n",
            "2333    1  37.8  31.6  1018.6    9.9  13.3   17.1  21.0  46.0  25.0  0.00   \n",
            "4127    4  37.7  24.6  1017.9    8.2  19.9   27.0  42.9  51.1  30.0  0.24   \n",
            "2973    1  68.3  64.1  1020.1   10.0  11.2   19.0  22.0  75.0  50.0  0.00   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_7.fit(\n",
        "    training_features15,\n",
        "    training_labels15,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSwhPbUik75F",
        "outputId": "ae9fe97c-d153-4852-c338-f03d71972f3f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.8 s, sys: 799 ms, total: 16.6 s\n",
            "Wall time: 16 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_7_results = dnn_model_7.evaluate(test_features15, test_labels15, verbose=0)\n",
        "print(dnn_model_7_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_7 = dnn_model_7.predict(valid_features15)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set15a = valid_set15.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set15a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_7.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set15a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYIhiU_3k78W",
        "outputId": "78324bb6-18eb-48bd-eb0e-404661d77e6a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14263370633125305\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  \\\n",
            "1504    6  41.2  37.6  1017.1    9.1   7.5   14.0  20.0  48.9  37.9  0.10   \n",
            "2408    7  69.4  59.9  1014.4    9.5  11.8   17.1  22.9  75.0  64.0  0.00   \n",
            "3567    7  39.4  34.8  1021.0    8.3  12.4   21.0  29.9  45.0  33.1  0.55   \n",
            "1720    4  61.0  61.0  1005.3    4.2   8.6   15.0  22.0  66.9  54.0  0.03   \n",
            "2977    6  61.4  58.0  1019.9    6.5  11.1   19.0  28.0  66.9  52.0  0.08   \n",
            "...   ...   ...   ...     ...    ...   ...    ...   ...   ...   ...   ...   \n",
            "4357    4  38.9  36.3  1010.4    4.0  22.0   34.0  45.1  46.9  26.1  0.20   \n",
            "3720    7  56.5  48.2  1026.6   10.0  15.5   22.0  25.1  63.0  51.1  0.00   \n",
            "883     7  63.8  50.3  1015.4    9.0   6.4   12.0  15.9  72.0  55.4  0.22   \n",
            "1273    6  75.7  65.2  1023.6    7.3   3.2   13.0  18.1  84.0  62.6  0.01   \n",
            "3173    4  30.9  10.8  1015.8   10.0  14.5   20.0  27.0  37.0  25.0  0.00   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504             181      [380.2561340332031]  \n",
            "2408             519       [512.566650390625]  \n",
            "3567             472      [414.2210693359375]  \n",
            "1720             279     [464.18963623046875]  \n",
            "2977             328      [387.5918273925781]  \n",
            "...              ...                      ...  \n",
            "4357             233      [284.6593017578125]  \n",
            "3720             432        [533.26220703125]  \n",
            "883              480       [456.819580078125]  \n",
            "1273             474       [530.813232421875]  \n",
            "3173             666      [653.1804809570312]  \n",
            "\n",
            "[136 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Mean Wind Speed"
      ],
      "metadata": {
        "id": "vndd1nBGjAFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data16 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers16 = [\"day\", \"temp\", \"dewp\", \"slp\", \"visib\", \"mxpsd\", \"gust\", \"max\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input16 = pd.concat(df_input_data16, axis=1, keys=df_input_headers16)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input16 = df_input16.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input16[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set16 = df_input16.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set16 = df_input16.drop(training_set16.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set16 = df_input16.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features16 = training_set16.copy()\n",
        "valid_features16 = valid_set16.copy()\n",
        "test_features16 = test_set16.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels16 = training_features16.pop('NUM_COLLISIONS')\n",
        "test_labels16 = test_features16.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features16.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels16 = training_labels16/SCALE_NUM_COLLISIONS\n",
        "test_labels16 = test_labels16/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features16))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_8 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_8.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0Nn4hc0k8iH",
        "outputId": "73ec7b1b-2fef-4997-9474-b28febe146b7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp     slp  visib  mxpsd  gust   max   min  prcp  fog  \\\n",
            "2807    7  62.2  56.2  1007.9    8.8   18.1  22.9  66.0  57.9  0.01    1   \n",
            "2288    3  59.8  55.0  1014.3    9.6   15.9  21.0  66.0  51.1  0.00    0   \n",
            "3995    2  48.3  45.7  1010.6    3.4   25.1  35.0  54.0  41.0  0.57    1   \n",
            "2333    1  37.8  31.6  1018.6    9.9   17.1  21.0  46.0  25.0  0.00    0   \n",
            "4127    4  37.7  24.6  1017.9    8.2   27.0  42.9  51.1  30.0  0.24    0   \n",
            "2973    1  68.3  64.1  1020.1   10.0   19.0  22.0  75.0  50.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_8.fit(\n",
        "    training_features16,\n",
        "    training_labels16,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyZeYBdOk8oX",
        "outputId": "9116cc73-7d22-406c-f5b5-2773d69bea29"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.2 s, sys: 833 ms, total: 17 s\n",
            "Wall time: 16.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_8_results = dnn_model_8.evaluate(test_features16, test_labels16, verbose=0)\n",
        "print(dnn_model_8_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_8 = dnn_model_8.predict(valid_features16)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set16a = valid_set16.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set16a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_8.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set16a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB_qNli5k8rg",
        "outputId": "4eff4bd6-c00e-4cde-eb03-cafc65a1f38e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13620688021183014\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "      day  temp  dewp     slp  visib  mxpsd  gust   max   min  prcp  fog  \\\n",
            "1504    6  41.2  37.6  1017.1    9.1   14.0  20.0  48.9  37.9  0.10    0   \n",
            "2408    7  69.4  59.9  1014.4    9.5   17.1  22.9  75.0  64.0  0.00    0   \n",
            "3567    7  39.4  34.8  1021.0    8.3   21.0  29.9  45.0  33.1  0.55    0   \n",
            "1720    4  61.0  61.0  1005.3    4.2   15.0  22.0  66.9  54.0  0.03    1   \n",
            "2977    6  61.4  58.0  1019.9    6.5   19.0  28.0  66.9  52.0  0.08    0   \n",
            "...   ...   ...   ...     ...    ...    ...   ...   ...   ...   ...  ...   \n",
            "4357    4  38.9  36.3  1010.4    4.0   34.0  45.1  46.9  26.1  0.20    0   \n",
            "3720    7  56.5  48.2  1026.6   10.0   22.0  25.1  63.0  51.1  0.00    0   \n",
            "883     7  63.8  50.3  1015.4    9.0   12.0  15.9  72.0  55.4  0.22    0   \n",
            "1273    6  75.7  65.2  1023.6    7.3   13.0  18.1  84.0  62.6  0.01    1   \n",
            "3173    4  30.9  10.8  1015.8   10.0   20.0  27.0  37.0  25.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504             181      [395.7192077636719]  \n",
            "2408             519      [479.0054016113281]  \n",
            "3567             472      [429.8802795410156]  \n",
            "1720             279      [459.1210021972656]  \n",
            "2977             328       [501.000244140625]  \n",
            "...              ...                      ...  \n",
            "4357             233     [269.32916259765625]  \n",
            "3720             432      [482.7241516113281]  \n",
            "883              480     [458.64056396484375]  \n",
            "1273             474      [508.9934387207031]  \n",
            "3173             666       [573.560791015625]  \n",
            "\n",
            "[136 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Maximum Sustained Wind Speed"
      ],
      "metadata": {
        "id": "ZeDBRlKejANs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data17 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers17 = [\"day\", \"temp\", \"dewp\", \"slp\", \"visib\", \"wdsp\", \"gust\", \"max\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input17 = pd.concat(df_input_data17, axis=1, keys=df_input_headers17)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input17 = df_input17.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input17[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set17 = df_input17.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set17 = df_input17.drop(training_set17.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set17 = df_input17.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features17 = training_set17.copy()\n",
        "valid_features17 = valid_set17.copy()\n",
        "test_features17 = test_set17.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels17 = training_features17.pop('NUM_COLLISIONS')\n",
        "test_labels17 = test_features17.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features17.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels17 = training_labels17/SCALE_NUM_COLLISIONS\n",
        "test_labels17 = test_labels17/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features17))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_9 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_9.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFQdcaI-k9Ju",
        "outputId": "f7d1d762-f101-43a5-f4c5-1c5cf385b408"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp     slp  visib  wdsp  gust   max   min  prcp  fog  \\\n",
            "2807    7  62.2  56.2  1007.9    8.8  12.3  22.9  66.0  57.9  0.01    1   \n",
            "2288    3  59.8  55.0  1014.3    9.6  12.3  21.0  66.0  51.1  0.00    0   \n",
            "3995    2  48.3  45.7  1010.6    3.4  17.3  35.0  54.0  41.0  0.57    1   \n",
            "2333    1  37.8  31.6  1018.6    9.9  13.3  21.0  46.0  25.0  0.00    0   \n",
            "4127    4  37.7  24.6  1017.9    8.2  19.9  42.9  51.1  30.0  0.24    0   \n",
            "2973    1  68.3  64.1  1020.1   10.0  11.2  22.0  75.0  50.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_9.fit(\n",
        "    training_features17,\n",
        "    training_labels17,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyqWLPU1k9M2",
        "outputId": "a8328147-daee-47ca-b818-870b7809b58e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.8 s, sys: 796 ms, total: 16.6 s\n",
            "Wall time: 15.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_9_results = dnn_model_9.evaluate(test_features17, test_labels17, verbose=0)\n",
        "print(dnn_model_9_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_9 = dnn_model_9.predict(valid_features17)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set17a = valid_set17.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set17a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_9.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set17a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IXd2gb2k9P9",
        "outputId": "f931ef76-1af7-4445-bcf8-83c14033bccf"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14368751645088196\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "      day  temp  dewp     slp  visib  wdsp  gust   max   min  prcp  fog  \\\n",
            "1504    6  41.2  37.6  1017.1    9.1   7.5  20.0  48.9  37.9  0.10    0   \n",
            "2408    7  69.4  59.9  1014.4    9.5  11.8  22.9  75.0  64.0  0.00    0   \n",
            "3567    7  39.4  34.8  1021.0    8.3  12.4  29.9  45.0  33.1  0.55    0   \n",
            "1720    4  61.0  61.0  1005.3    4.2   8.6  22.0  66.9  54.0  0.03    1   \n",
            "2977    6  61.4  58.0  1019.9    6.5  11.1  28.0  66.9  52.0  0.08    0   \n",
            "...   ...   ...   ...     ...    ...   ...   ...   ...   ...   ...  ...   \n",
            "4357    4  38.9  36.3  1010.4    4.0  22.0  45.1  46.9  26.1  0.20    0   \n",
            "3720    7  56.5  48.2  1026.6   10.0  15.5  25.1  63.0  51.1  0.00    0   \n",
            "883     7  63.8  50.3  1015.4    9.0   6.4  15.9  72.0  55.4  0.22    0   \n",
            "1273    6  75.7  65.2  1023.6    7.3   3.2  18.1  84.0  62.6  0.01    1   \n",
            "3173    4  30.9  10.8  1015.8   10.0  14.5  27.0  37.0  25.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504             181      [361.2194519042969]  \n",
            "2408             519        [438.69970703125]  \n",
            "3567             472       [415.325439453125]  \n",
            "1720             279      [599.4387817382812]  \n",
            "2977             328      [301.2900695800781]  \n",
            "...              ...                      ...  \n",
            "4357             233      [89.74996948242188]  \n",
            "3720             432          [411.912109375]  \n",
            "883              480      [463.5356140136719]  \n",
            "1273             474     [462.24566650390625]  \n",
            "3173             666      [631.5972290039062]  \n",
            "\n",
            "[136 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Maximum Wind Gust"
      ],
      "metadata": {
        "id": "iinQsZnpkIkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data18 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers18 = [\"day\", \"temp\", \"dewp\", \"slp\", \"visib\", \"wdsp\", \"mxpsd\", \"max\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input18 = pd.concat(df_input_data18, axis=1, keys=df_input_headers18)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input18 = df_input18.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input18[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set18 = df_input18.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set18 = df_input18.drop(training_set18.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set18 = df_input18.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features18 = training_set18.copy()\n",
        "valid_features18 = valid_set18.copy()\n",
        "test_features18 = test_set18.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels18 = training_features18.pop('NUM_COLLISIONS')\n",
        "test_labels18 = test_features18.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features18.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels18 = training_labels18/SCALE_NUM_COLLISIONS\n",
        "test_labels18 = test_labels18/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features18))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_10 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_10.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZEZchmOk903",
        "outputId": "ef8c333a-def0-4faa-b005-a21d61a69232"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp     slp  visib  wdsp  mxpsd   max   min  prcp  fog  \\\n",
            "1785    3  62.9  51.9  1019.8   10.0   8.2   15.0  73.9  57.0  0.57    0   \n",
            "2807    7  62.2  56.2  1007.9    8.8  12.3   18.1  66.0  57.9  0.01    1   \n",
            "2476    5  66.9  63.6  1017.1    7.8   8.7   17.1  77.0  61.0  0.00    1   \n",
            "2288    3  59.8  55.0  1014.3    9.6  12.3   15.9  66.0  51.1  0.00    0   \n",
            "1697    2  55.3  48.6  1022.7   10.0   9.6   15.0  64.0  46.0  0.00    0   \n",
            "640     1  32.7  23.9  1021.3    9.7   4.6   11.1  43.0  24.1  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "1785             686  \n",
            "2807             206  \n",
            "2476             752  \n",
            "2288             655  \n",
            "1697             720  \n",
            "640              706  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_10.fit(\n",
        "    training_features18,\n",
        "    training_labels18,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEKs-VpGk94Q",
        "outputId": "07c62406-a8f3-4de1-9e23-826604cb7000"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22.3 s, sys: 1.12 s, total: 23.4 s\n",
            "Wall time: 27.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_10_results = dnn_model_10.evaluate(test_features18, test_labels18, verbose=0)\n",
        "print(dnn_model_10_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_10 = dnn_model_10.predict(valid_features18)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set18a = valid_set18.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set18a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_10.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set18a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH8-FDGck97d",
        "outputId": "cd35bcbf-6ae5-469e-e7c7-b35c81130779"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1391804963350296\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "      day  temp  dewp     slp  visib  wdsp  mxpsd   max   min  prcp  fog  \\\n",
            "2613    3  64.8  58.1  1012.1    9.3   8.7   17.1  73.9  53.1  0.01    1   \n",
            "2236    1  32.1  15.5  1027.6   10.0   9.0   15.9  37.0  26.1  0.00    0   \n",
            "1220    6  42.6  27.1  1024.5    9.9   2.8   13.0  46.9  33.8  0.00    0   \n",
            "3195    1  52.6  45.8  1019.0    8.2  15.3   20.0  61.0  34.0  0.01    0   \n",
            "3218    6  50.7  49.8  1013.7    3.6  15.2   20.0  59.0  50.0  0.07    1   \n",
            "...   ...   ...   ...     ...    ...   ...    ...   ...   ...   ...  ...   \n",
            "2165    2  56.1  53.0  1008.2    5.9   8.0   15.9  66.0  46.9  0.01    1   \n",
            "3846    1  54.9  45.0  1017.4   10.0  16.0   22.9  59.0  51.1  0.00    0   \n",
            "1096    4  54.0  39.9  1025.2   10.0   6.9   13.0  66.9  45.0  0.00    0   \n",
            "2688    7  63.1  56.2  1020.3    9.4   9.1   18.1  78.1  50.0  0.00    0   \n",
            "431     4  70.4  54.8  1027.4   10.0   3.2    9.9  80.1  62.1  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "2613             647      [635.8245849609375]  \n",
            "2236             623      [624.3683471679688]  \n",
            "1220             523      [502.5155029296875]  \n",
            "3195             345       [718.668212890625]  \n",
            "3218             265      [515.9767456054688]  \n",
            "...              ...                      ...  \n",
            "2165             281     [368.01739501953125]  \n",
            "3846             329     [428.63409423828125]  \n",
            "1096             654      [549.7174682617188]  \n",
            "2688             543      [497.9642639160156]  \n",
            "431              632       [721.726806640625]  \n",
            "\n",
            "[219 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Total Precipitation"
      ],
      "metadata": {
        "id": "Aph8rE9QkInj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data19 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"],\n",
        "                   df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"fog\"], df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers19 = [\"day\", \"temp\", \"dewp\", \"slp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"max\", \"min\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input19 = pd.concat(df_input_data19, axis=1, keys=df_input_headers19)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input19 = df_input19.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input19[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set19 = df_input19.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set19 = df_input19.drop(training_set19.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set19 = df_input19.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features19 = training_set19.copy()\n",
        "valid_features19 = valid_set19.copy()\n",
        "test_features19 = test_set19.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels19 = training_features19.pop('NUM_COLLISIONS')\n",
        "test_labels19 = test_features19.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features19.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels19 = training_labels19/SCALE_NUM_COLLISIONS\n",
        "test_labels19 = test_labels19/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features19))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_11 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_11.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F8xUJJ7k-Zt",
        "outputId": "fad1614f-1e8a-4170-ea6c-1e29161e205a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   max   min  fog  \\\n",
            "2807    7  62.2  56.2  1007.9    8.8  12.3   18.1  22.9  66.0  57.9    1   \n",
            "2288    3  59.8  55.0  1014.3    9.6  12.3   15.9  21.0  66.0  51.1    0   \n",
            "3995    2  48.3  45.7  1010.6    3.4  17.3   25.1  35.0  54.0  41.0    1   \n",
            "2333    1  37.8  31.6  1018.6    9.9  13.3   17.1  21.0  46.0  25.0    0   \n",
            "4127    4  37.7  24.6  1017.9    8.2  19.9   27.0  42.9  51.1  30.0    0   \n",
            "2973    1  68.3  64.1  1020.1   10.0  11.2   19.0  22.0  75.0  50.0    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_11.fit(\n",
        "    training_features19,\n",
        "    training_labels19,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPOdWOt6k-dU",
        "outputId": "64980ae6-0837-4d89-b4d1-7ab6e44ef1dd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.4 s, sys: 769 ms, total: 17.2 s\n",
            "Wall time: 17.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_11_results = dnn_model_11.evaluate(test_features19, test_labels19, verbose=0)\n",
        "print(dnn_model_11_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_11 = dnn_model_11.predict(valid_features19)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set19a = valid_set19.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set19a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_11.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set19a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG5PTiC-k-gF",
        "outputId": "0d2066d4-075f-4909-fb78-89c1d65055b9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13887201249599457\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "      day  temp  dewp     slp  visib  wdsp  mxpsd  gust   max   min  fog  \\\n",
            "3271    1  62.3  58.2  1011.6    8.8  13.8   20.0  27.0  68.0  55.0    0   \n",
            "3709    7  48.8  39.8  1014.2    9.6  11.9   22.0  28.9  54.0  44.6    0   \n",
            "3135    7  49.6  47.9  1020.7    6.0  15.3   19.0  28.0  54.0  44.1    1   \n",
            "4263    4  43.7  39.1  1021.6    5.3  16.4   28.9  36.9  50.0  35.1    1   \n",
            "1746    4  42.7  35.1  1012.7    9.8   6.4   15.0  22.9  46.9  37.0    0   \n",
            "...   ...   ...   ...     ...    ...   ...    ...   ...   ...   ...  ...   \n",
            "2771    2  63.3  59.7  1021.0    8.7  14.1   18.1  22.9  66.9  61.0    0   \n",
            "2355    2  37.1  22.6  1013.5   10.0  12.2   17.1  24.1  48.0  32.0    0   \n",
            "2506    5  53.6  44.3  1015.1   10.0  12.1   17.1  22.9  63.0  48.9    0   \n",
            "3595    7  37.8  34.1  1026.4    9.1  13.4   22.0  28.0  46.0  30.0    0   \n",
            "2922    3  42.1  38.5  1012.8    5.0  12.3   19.0  25.1  48.0  28.0    1   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "3271             641      [626.3776245117188]  \n",
            "3709             241     [312.50238037109375]  \n",
            "3135             256       [360.459716796875]  \n",
            "4263             266      [438.4338073730469]  \n",
            "1746             635      [470.5888366699219]  \n",
            "...              ...                      ...  \n",
            "2771             318      [467.4805603027344]  \n",
            "2355             282     [424.03363037109375]  \n",
            "2506             777       [577.367919921875]  \n",
            "3595             541       [490.489990234375]  \n",
            "2922             565      [585.4888305664062]  \n",
            "\n",
            "[137 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Mean Visibility"
      ],
      "metadata": {
        "id": "rYlAUrkJkIq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data20 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers20 = [\"day\", \"temp\", \"dewp\", \"slp\", \"wdsp\", \"mxpsd\", \"gust\", \"max\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input20 = pd.concat(df_input_data20, axis=1, keys=df_input_headers20)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input20 = df_input20.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input20[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set20 = df_input20.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set20 = df_input20.drop(training_set20.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set20 = df_input20.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features20 = training_set20.copy()\n",
        "valid_features20 = valid_set20.copy()\n",
        "test_features20 = test_set20.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels20 = training_features20.pop('NUM_COLLISIONS')\n",
        "test_labels20 = test_features20.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features20.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels20 = training_labels20/SCALE_NUM_COLLISIONS\n",
        "test_labels20 = test_labels20/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features20))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_12 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_12.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mko7NO9lBIA",
        "outputId": "8c1e3d3f-12e5-46c1-918f-a376ecc96e0a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp     slp  wdsp  mxpsd  gust   max   min  prcp  fog  \\\n",
            "2807    7  62.2  56.2  1007.9  12.3   18.1  22.9  66.0  57.9  0.01    1   \n",
            "2288    3  59.8  55.0  1014.3  12.3   15.9  21.0  66.0  51.1  0.00    0   \n",
            "3995    2  48.3  45.7  1010.6  17.3   25.1  35.0  54.0  41.0  0.57    1   \n",
            "2333    1  37.8  31.6  1018.6  13.3   17.1  21.0  46.0  25.0  0.00    0   \n",
            "4127    4  37.7  24.6  1017.9  19.9   27.0  42.9  51.1  30.0  0.24    0   \n",
            "2973    1  68.3  64.1  1020.1  11.2   19.0  22.0  75.0  50.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_12.fit(\n",
        "    training_features20,\n",
        "    training_labels20,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeykgqWAlBN2",
        "outputId": "e620f5bc-d1ec-4cda-be05-79941e697743"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.7 s, sys: 834 ms, total: 16.5 s\n",
            "Wall time: 16.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_12_results = dnn_model_12.evaluate(test_features20, test_labels20, verbose=0)\n",
        "print(dnn_model_12_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_12 = dnn_model_12.predict(valid_features20)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set20a = valid_set20.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set20a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_12.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set20a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZON4lXBJlBUq",
        "outputId": "1211c0f8-7ec2-4254-82f2-9f8476b23496"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14489327371120453\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "      day  temp  dewp     slp  wdsp  mxpsd  gust   max   min  prcp  fog  \\\n",
            "1504    6  41.2  37.6  1017.1   7.5   14.0  20.0  48.9  37.9  0.10    0   \n",
            "2408    7  69.4  59.9  1014.4  11.8   17.1  22.9  75.0  64.0  0.00    0   \n",
            "3567    7  39.4  34.8  1021.0  12.4   21.0  29.9  45.0  33.1  0.55    0   \n",
            "1720    4  61.0  61.0  1005.3   8.6   15.0  22.0  66.9  54.0  0.03    1   \n",
            "2977    6  61.4  58.0  1019.9  11.1   19.0  28.0  66.9  52.0  0.08    0   \n",
            "...   ...   ...   ...     ...   ...    ...   ...   ...   ...   ...  ...   \n",
            "4357    4  38.9  36.3  1010.4  22.0   34.0  45.1  46.9  26.1  0.20    0   \n",
            "3720    7  56.5  48.2  1026.6  15.5   22.0  25.1  63.0  51.1  0.00    0   \n",
            "883     7  63.8  50.3  1015.4   6.4   12.0  15.9  72.0  55.4  0.22    0   \n",
            "1273    6  75.7  65.2  1023.6   3.2   13.0  18.1  84.0  62.6  0.01    1   \n",
            "3173    4  30.9  10.8  1015.8  14.5   20.0  27.0  37.0  25.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504             181      [409.8791198730469]  \n",
            "2408             519     [511.61212158203125]  \n",
            "3567             472          [454.748046875]  \n",
            "1720             279      [400.1839294433594]  \n",
            "2977             328       [454.083251953125]  \n",
            "...              ...                      ...  \n",
            "4357             233     [210.17507934570312]  \n",
            "3720             432      [458.9444885253906]  \n",
            "883              480      [525.8213500976562]  \n",
            "1273             474      [501.2203369140625]  \n",
            "3173             666      [620.2044067382812]  \n",
            "\n",
            "[136 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove Day of Week"
      ],
      "metadata": {
        "id": "WdRRQUPUkfrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data21 = [df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"slp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"], df_shuffle[\"max\"], df_shuffle[\"min\"], df_shuffle[\"prcp\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers21 = [\"temp\", \"dewp\", \"slp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"max\", \"min\", \"prcp\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input21 = pd.concat(df_input_data21, axis=1, keys=df_input_headers21)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input21 = df_input21.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input21[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set21 = df_input21.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set21 = df_input21.drop(training_set21.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set21 = df_input21.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features21 = training_set21.copy()\n",
        "valid_features21 = valid_set21.copy()\n",
        "test_features21 = test_set21.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels21 = training_features21.pop('NUM_COLLISIONS')\n",
        "test_labels21 = test_features21.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features21.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels21 = training_labels21/SCALE_NUM_COLLISIONS\n",
        "test_labels21 = test_labels21/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features21))\n",
        "\n",
        "# Normalisation layer (11 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_13 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_13.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVbEtchelB0f",
        "outputId": "e2452cce-b90d-4bc9-8b36-912bb06a30e6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      temp  dewp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  fog  \\\n",
            "2807  62.2  56.2  1007.9    8.8  12.3   18.1  22.9  66.0  57.9  0.01    1   \n",
            "2288  59.8  55.0  1014.3    9.6  12.3   15.9  21.0  66.0  51.1  0.00    0   \n",
            "3995  48.3  45.7  1010.6    3.4  17.3   25.1  35.0  54.0  41.0  0.57    1   \n",
            "2333  37.8  31.6  1018.6    9.9  13.3   17.1  21.0  46.0  25.0  0.00    0   \n",
            "4127  37.7  24.6  1017.9    8.2  19.9   27.0  42.9  51.1  30.0  0.24    0   \n",
            "2973  68.3  64.1  1020.1   10.0  11.2   19.0  22.0  75.0  50.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS  \n",
            "2807             206  \n",
            "2288             655  \n",
            "3995             608  \n",
            "2333             238  \n",
            "4127             252  \n",
            "2973             521  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_13.fit(\n",
        "    training_features21,\n",
        "    training_labels21,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1JMri6olB3e",
        "outputId": "d7d15465-d37f-42e4-f6ca-d8274a7d6455"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.7 s, sys: 787 ms, total: 16.5 s\n",
            "Wall time: 16.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_13_results = dnn_model_13.evaluate(test_features21, test_labels21, verbose=0)\n",
        "print(dnn_model_13_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_13 = dnn_model_13.predict(valid_features21)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set21a = valid_set21.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set21a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_13.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set21a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M5EHUeelCAf",
        "outputId": "43baa0cf-d3e7-4864-a6fd-16b6d649bd45"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.143428772687912\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "      temp  dewp     slp  visib  wdsp  mxpsd  gust   max   min  prcp  fog  \\\n",
            "1504  41.2  37.6  1017.1    9.1   7.5   14.0  20.0  48.9  37.9  0.10    0   \n",
            "2408  69.4  59.9  1014.4    9.5  11.8   17.1  22.9  75.0  64.0  0.00    0   \n",
            "3567  39.4  34.8  1021.0    8.3  12.4   21.0  29.9  45.0  33.1  0.55    0   \n",
            "1720  61.0  61.0  1005.3    4.2   8.6   15.0  22.0  66.9  54.0  0.03    1   \n",
            "2977  61.4  58.0  1019.9    6.5  11.1   19.0  28.0  66.9  52.0  0.08    0   \n",
            "...    ...   ...     ...    ...   ...    ...   ...   ...   ...   ...  ...   \n",
            "4357  38.9  36.3  1010.4    4.0  22.0   34.0  45.1  46.9  26.1  0.20    0   \n",
            "3720  56.5  48.2  1026.6   10.0  15.5   22.0  25.1  63.0  51.1  0.00    0   \n",
            "883   63.8  50.3  1015.4    9.0   6.4   12.0  15.9  72.0  55.4  0.22    0   \n",
            "1273  75.7  65.2  1023.6    7.3   3.2   13.0  18.1  84.0  62.6  0.01    1   \n",
            "3173  30.9  10.8  1015.8   10.0  14.5   20.0  27.0  37.0  25.0  0.00    0   \n",
            "\n",
            "      NUM_COLLISIONS NUM_COLLISIONS_predicted  \n",
            "1504             181      [464.1197814941406]  \n",
            "2408             519      [484.5956726074219]  \n",
            "3567             472        [482.87744140625]  \n",
            "1720             279       [431.240478515625]  \n",
            "2977             328      [487.2491149902344]  \n",
            "...              ...                      ...  \n",
            "4357             233     [225.06680297851562]  \n",
            "3720             432       [511.717529296875]  \n",
            "883              480      [586.4413452148438]  \n",
            "1273             474      [475.9161682128906]  \n",
            "3173             666      [644.0860595703125]  \n",
            "\n",
            "[136 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DNN Model with Significant Variables from Linear Model"
      ],
      "metadata": {
        "id": "0GxBcYQ_ljMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data22 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"wdsp\"], df_shuffle[\"mxpsd\"], df_shuffle[\"gust\"], df_shuffle[\"prcp\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers22 = [\"day\", \"temp\", \"dewp\", \"visib\", \"wdsp\", \"mxpsd\", \"gust\", \"prcp\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input22 = pd.concat(df_input_data22, axis=1, keys=df_input_headers22)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input22 = df_input22.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input22[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set22 = df_input22.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set22 = df_input22.drop(training_set22.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set22 = df_input22.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features22 = training_set22.copy()\n",
        "valid_features22 = valid_set22.copy()\n",
        "test_features22 = test_set22.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels22 = training_features22.pop('NUM_COLLISIONS')\n",
        "test_labels22 = test_features22.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features22.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels22 = training_labels22/SCALE_NUM_COLLISIONS\n",
        "test_labels22 = test_labels22/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features22))\n",
        "\n",
        "# Normalisation layer (8 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_14 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_14.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InydOZPghi2k",
        "outputId": "f81b1a35-815b-417a-fa5d-69e5665757c6"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  visib  wdsp  mxpsd  gust  prcp  NUM_COLLISIONS\n",
            "2807    7  62.2  56.2    8.8  12.3   18.1  22.9  0.01             206\n",
            "2288    3  59.8  55.0    9.6  12.3   15.9  21.0  0.00             655\n",
            "3995    2  48.3  45.7    3.4  17.3   25.1  35.0  0.57             608\n",
            "2333    1  37.8  31.6    9.9  13.3   17.1  21.0  0.00             238\n",
            "4127    4  37.7  24.6    8.2  19.9   27.0  42.9  0.24             252\n",
            "2973    1  68.3  64.1   10.0  11.2   19.0  22.0  0.00             521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_14.fit(\n",
        "    training_features22,\n",
        "    training_labels22,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGsdNbj8hi_z",
        "outputId": "e9cab875-dd10-4117-f1ef-effcf75496d4"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.7 s, sys: 813 ms, total: 17.5 s\n",
            "Wall time: 16.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_14_results = dnn_model_14.evaluate(test_features22, test_labels22, verbose=0)\n",
        "print(dnn_model_14_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_14 = dnn_model_14.predict(valid_features22)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set22a = valid_set22.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set22a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_14.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set22a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMJ7naUyhjHy",
        "outputId": "3ef87037-c74b-4437-fee8-2b89497bd4d9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1338409036397934\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "      day  temp  dewp  visib  wdsp  mxpsd  gust  prcp  NUM_COLLISIONS  \\\n",
            "1504    6  41.2  37.6    9.1   7.5   14.0  20.0  0.10             181   \n",
            "2408    7  69.4  59.9    9.5  11.8   17.1  22.9  0.00             519   \n",
            "3567    7  39.4  34.8    8.3  12.4   21.0  29.9  0.55             472   \n",
            "1720    4  61.0  61.0    4.2   8.6   15.0  22.0  0.03             279   \n",
            "2977    6  61.4  58.0    6.5  11.1   19.0  28.0  0.08             328   \n",
            "...   ...   ...   ...    ...   ...    ...   ...   ...             ...   \n",
            "4357    4  38.9  36.3    4.0  22.0   34.0  45.1  0.20             233   \n",
            "3720    7  56.5  48.2   10.0  15.5   22.0  25.1  0.00             432   \n",
            "883     7  63.8  50.3    9.0   6.4   12.0  15.9  0.22             480   \n",
            "1273    6  75.7  65.2    7.3   3.2   13.0  18.1  0.01             474   \n",
            "3173    4  30.9  10.8   10.0  14.5   20.0  27.0  0.00             666   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "1504      [439.2330322265625]  \n",
            "2408      [488.7619934082031]  \n",
            "3567     [376.24847412109375]  \n",
            "1720      [568.5155029296875]  \n",
            "2977        [426.07275390625]  \n",
            "...                       ...  \n",
            "4357      [383.2090759277344]  \n",
            "3720      [462.3994445800781]  \n",
            "883       [560.0271606445312]  \n",
            "1273      [554.0069580078125]  \n",
            "3173      [576.6203002929688]  \n",
            "\n",
            "[136 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove All Variables Whose Removal Decreased Mean Absolute Error"
      ],
      "metadata": {
        "id": "-x3gI664nI6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data23 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"mxpsd\"], df_shuffle[\"max\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers23 = [\"day\", \"temp\", \"dewp\", \"visib\", \"mxpsd\", \"max\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input23 = pd.concat(df_input_data23, axis=1, keys=df_input_headers23)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input23 = df_input23.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input23[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set23 = df_input23.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set23 = df_input23.drop(training_set23.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set23 = df_input23.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features23 = training_set23.copy()\n",
        "valid_features23 = valid_set23.copy()\n",
        "test_features23 = test_set23.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels23 = training_features23.pop('NUM_COLLISIONS')\n",
        "test_labels23 = test_features23.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features23.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels23 = training_labels23/SCALE_NUM_COLLISIONS\n",
        "test_labels23 = test_labels23/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features23))\n",
        "\n",
        "# Normalisation layer (7 inputs), 2 layers of 48, with 1 output.\n",
        "dnn_model_15 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_15.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqYDzJdmnG3m",
        "outputId": "11ff50d1-6757-4f81-fd70-03887f5e9a0f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  visib  mxpsd   max  fog  NUM_COLLISIONS\n",
            "1785    3  62.9  51.9   10.0   15.0  73.9    0             686\n",
            "2807    7  62.2  56.2    8.8   18.1  66.0    1             206\n",
            "2476    5  66.9  63.6    7.8   17.1  77.0    1             752\n",
            "2288    3  59.8  55.0    9.6   15.9  66.0    0             655\n",
            "1697    2  55.3  48.6   10.0   15.0  64.0    0             720\n",
            "640     1  32.7  23.9    9.7   11.1  43.0    0             706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_15.fit(\n",
        "    training_features23,\n",
        "    training_labels23,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBtPZ0BgnIQK",
        "outputId": "1640bea4-95ac-4f29-a9b8-0134e94e9cfa"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22.3 s, sys: 1.09 s, total: 23.4 s\n",
            "Wall time: 27 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_15_results = dnn_model_15.evaluate(test_features23, test_labels23, verbose=0)\n",
        "print(dnn_model_15_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_15 = dnn_model_15.predict(valid_features23)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set23a = valid_set23.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set23a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_15.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set23a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7masMN8nITj",
        "outputId": "ac71f532-74c4-4444-d8fc-fff9e7f63e67"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13062207400798798\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "      day  temp  dewp  visib  mxpsd   max  fog  NUM_COLLISIONS  \\\n",
            "3727    4  24.9   4.8   10.0   22.9  44.1    0             412   \n",
            "3834    7  51.9  51.2    2.1   22.9  55.9    1             505   \n",
            "2859    6  44.9  36.6    9.4   18.1  55.0    0             549   \n",
            "3822    2  32.6  25.6    8.4   22.9  52.0    0             613   \n",
            "3578    2  60.5  55.1    8.7   21.0  64.9    0             715   \n",
            "...   ...   ...   ...    ...    ...   ...  ...             ...   \n",
            "3537    4  46.6  41.2   10.0   21.0  55.9    0             197   \n",
            "2914    4  29.9  11.8   10.0   19.0  36.0    0             625   \n",
            "2765    5  61.7  58.6    4.0   18.1  66.0    1             708   \n",
            "2248    7  71.5  70.6    7.0   15.9  77.0    1             284   \n",
            "2010    3  41.1  38.4    6.5   15.9  48.9    0             583   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3727      [620.3088989257812]  \n",
            "3834     [371.98004150390625]  \n",
            "2859      [535.9517822265625]  \n",
            "3822      [487.8047790527344]  \n",
            "3578        [573.37255859375]  \n",
            "...                       ...  \n",
            "3537     [355.37896728515625]  \n",
            "2914      [582.4754028320312]  \n",
            "2765      [603.1582641601562]  \n",
            "2248      [285.5023498535156]  \n",
            "2010      [501.8629455566406]  \n",
            "\n",
            "[221 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove All Variables Whose Removal Decreased Mean Absolute Error AND Increase Neurons in Dense Layer"
      ],
      "metadata": {
        "id": "wSQ39c8LJ1Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data24 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"mxpsd\"], df_shuffle[\"max\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers24 = [\"day\", \"temp\", \"dewp\", \"visib\", \"mxpsd\", \"max\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input24 = pd.concat(df_input_data24, axis=1, keys=df_input_headers24)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input24 = df_input24.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input24[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set24 = df_input24.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set24 = df_input24.drop(training_set24.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set24 = df_input24.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features24 = training_set24.copy()\n",
        "valid_features24 = valid_set24.copy()\n",
        "test_features24 = test_set24.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels24 = training_features24.pop('NUM_COLLISIONS')\n",
        "test_labels24 = test_features24.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features24.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels24 = training_labels24/SCALE_NUM_COLLISIONS\n",
        "test_labels24 = test_labels24/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features24))\n",
        "\n",
        "# Normalisation layer (7 inputs), increase to 2 layers of 100, with 1 output.\n",
        "dnn_model_16 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(100, activation='relu'),\n",
        "      layers.Dense(100, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_16.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyvUhoBXJ1kd",
        "outputId": "f6144aeb-c7a7-4f00-f1a3-d45c0231d5d3"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  visib  mxpsd   max  fog  NUM_COLLISIONS\n",
            "1785    3  62.9  51.9   10.0   15.0  73.9    0             686\n",
            "2807    7  62.2  56.2    8.8   18.1  66.0    1             206\n",
            "2476    5  66.9  63.6    7.8   17.1  77.0    1             752\n",
            "2288    3  59.8  55.0    9.6   15.9  66.0    0             655\n",
            "1697    2  55.3  48.6   10.0   15.0  64.0    0             720\n",
            "640     1  32.7  23.9    9.7   11.1  43.0    0             706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_16.fit(\n",
        "    training_features24,\n",
        "    training_labels24,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB27zd0DJ1tr",
        "outputId": "8380c07e-783d-4e03-de10-a79018686a4b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 23.3 s, sys: 1.16 s, total: 24.5 s\n",
            "Wall time: 29 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_16_results = dnn_model_16.evaluate(test_features24, test_labels24, verbose=0)\n",
        "print(dnn_model_16_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_16 = dnn_model_16.predict(valid_features24)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set24a = valid_set24.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set24a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_16.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set24a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiNrjSYVJ12E",
        "outputId": "c8c96e5d-5044-4709-e4bd-b001c821aaa4"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13369134068489075\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "      day  temp  dewp  visib  mxpsd   max  fog  NUM_COLLISIONS  \\\n",
            "3727    4  24.9   4.8   10.0   22.9  44.1    0             412   \n",
            "3834    7  51.9  51.2    2.1   22.9  55.9    1             505   \n",
            "2859    6  44.9  36.6    9.4   18.1  55.0    0             549   \n",
            "3822    2  32.6  25.6    8.4   22.9  52.0    0             613   \n",
            "3578    2  60.5  55.1    8.7   21.0  64.9    0             715   \n",
            "...   ...   ...   ...    ...    ...   ...  ...             ...   \n",
            "3537    4  46.6  41.2   10.0   21.0  55.9    0             197   \n",
            "2914    4  29.9  11.8   10.0   19.0  36.0    0             625   \n",
            "2765    5  61.7  58.6    4.0   18.1  66.0    1             708   \n",
            "2248    7  71.5  70.6    7.0   15.9  77.0    1             284   \n",
            "2010    3  41.1  38.4    6.5   15.9  48.9    0             583   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3727      [526.2708740234375]  \n",
            "3834      [448.9442443847656]  \n",
            "2859     [445.73138427734375]  \n",
            "3822      [426.2801208496094]  \n",
            "3578      [526.0506591796875]  \n",
            "...                       ...  \n",
            "3537     [402.14312744140625]  \n",
            "2914      [528.1480712890625]  \n",
            "2765        [679.98388671875]  \n",
            "2248     [444.92681884765625]  \n",
            "2010      [576.5208740234375]  \n",
            "\n",
            "[221 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove All Variables Whose Removal Decreased Mean Absolute Error AND Increase Number of Layers"
      ],
      "metadata": {
        "id": "_HALSIJVfHeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an input dataframe from the shuffled dataframe\n",
        "\n",
        "#get the data\n",
        "df_input_data25 = [df_shuffle[\"day\"], df_shuffle[\"temp\"], df_shuffle[\"dewp\"], df_shuffle[\"visib\"], df_shuffle[\"mxpsd\"], df_shuffle[\"max\"], df_shuffle[\"fog\"],\n",
        "                 df_shuffle[\"NUM_COLLISIONS\"]]\n",
        "\n",
        "# create headers\n",
        "df_input_headers25 = [\"day\", \"temp\", \"dewp\", \"visib\", \"mxpsd\", \"max\", \"fog\",\n",
        "                    \"NUM_COLLISIONS\"]\n",
        "\n",
        "# create a dataframe containing the data and headers\n",
        "df_input25 = pd.concat(df_input_data25, axis=1, keys=df_input_headers25)\n",
        "\n",
        "#drop the rows with NaN values (regression can't handle this)\n",
        "df_input25 = df_input25.dropna(axis=0)\n",
        "\n",
        "#print first 6 rows to check the data\n",
        "print(df_input25[:6])\n",
        "\n",
        "# create a training set, validation set, and a test set for the model. Sample 80% for the training set and remaining 20% for the test set\n",
        "\n",
        "#create a training set with fraction 80% of data, set random_state=0 for reproducibility\n",
        "training_set25 = df_input25.sample(frac=0.8, random_state=0)\n",
        "\n",
        "#create a test set with the remaining 20% of data, by dropping the 80% in the training set based on the index\n",
        "test_set25 = df_input25.drop(training_set25.index)\n",
        "\n",
        "#Also sample 5% for the validation set, set random_state=0 for reproducibility\n",
        "valid_set25 = df_input25.sample(frac=0.05, random_state=0)\n",
        "\n",
        "# create a copy of the datasets\n",
        "training_features25 = training_set25.copy()\n",
        "valid_features25 = valid_set25.copy()\n",
        "test_features25 = test_set25.copy()\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS), set it to X_labels for the test and training sets\n",
        "training_labels25 = training_features25.pop('NUM_COLLISIONS')\n",
        "test_labels25 = test_features25.pop('NUM_COLLISIONS')\n",
        "\n",
        "#use pop to remove the final column (NUM_COLLISIONS) from valid_features to create validation set\n",
        "valid_features25.pop('NUM_COLLISIONS')\n",
        "\n",
        "# divide by the scale factor (1200)\n",
        "training_labels25 = training_labels25/SCALE_NUM_COLLISIONS\n",
        "test_labels25 = test_labels25/SCALE_NUM_COLLISIONS\n",
        "\n",
        "normaliser = tf.keras.layers.Normalization(axis=-1)\n",
        "normaliser.adapt(np.array(training_features25))\n",
        "\n",
        "# Normalisation layer (7 inputs), 3 layers of 48, with 1 output.\n",
        "dnn_model_17 = keras.Sequential([\n",
        "      normaliser,\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(48, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model_17.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WIFYM30fHrU",
        "outputId": "eedfbace-2ebb-4dac-a6f4-bacc996ca39b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      day  temp  dewp  visib  mxpsd   max  fog  NUM_COLLISIONS\n",
            "1785    3  62.9  51.9   10.0   15.0  73.9    0             686\n",
            "2807    7  62.2  56.2    8.8   18.1  66.0    1             206\n",
            "2476    5  66.9  63.6    7.8   17.1  77.0    1             752\n",
            "2288    3  59.8  55.0    9.6   15.9  66.0    0             655\n",
            "1697    2  55.3  48.6   10.0   15.0  64.0    0             720\n",
            "640     1  32.7  23.9    9.7   11.1  43.0    0             706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model_17.fit(\n",
        "    training_features25,\n",
        "    training_labels25,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZL4Nf2PfHzY",
        "outputId": "ff43fcf8-fea1-4cd6-8c8e-a26995c78b1c"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 24.3 s, sys: 1.16 s, total: 25.5 s\n",
            "Wall time: 29.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the mean absolute error for the model\n",
        "dnn_model_17_results = dnn_model_17.evaluate(test_features25, test_labels25, verbose=0)\n",
        "print(dnn_model_17_results)\n",
        "\n",
        "# test the model on the validation set and compare predicted with actual\n",
        "dnn_day_predictions_17 = dnn_model_17.predict(valid_features25)*1200 #scale factor\n",
        "\n",
        "#create a copy of the validation set\n",
        "valid_set25a = valid_set25.copy()\n",
        "\n",
        "#add a column for the predicted number of collisions using our model\n",
        "valid_set25a[\"NUM_COLLISIONS_predicted\"] = dnn_day_predictions_17.tolist()\n",
        "\n",
        "#print the dataframe to compare actual and predicted data\n",
        "print(valid_set25a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_4m3OaafH70",
        "outputId": "d60a5348-5364-46c5-b080-8e12a85ee461"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1356942057609558\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "      day  temp  dewp  visib  mxpsd   max  fog  NUM_COLLISIONS  \\\n",
            "3727    4  24.9   4.8   10.0   22.9  44.1    0             412   \n",
            "3834    7  51.9  51.2    2.1   22.9  55.9    1             505   \n",
            "2859    6  44.9  36.6    9.4   18.1  55.0    0             549   \n",
            "3822    2  32.6  25.6    8.4   22.9  52.0    0             613   \n",
            "3578    2  60.5  55.1    8.7   21.0  64.9    0             715   \n",
            "...   ...   ...   ...    ...    ...   ...  ...             ...   \n",
            "3537    4  46.6  41.2   10.0   21.0  55.9    0             197   \n",
            "2914    4  29.9  11.8   10.0   19.0  36.0    0             625   \n",
            "2765    5  61.7  58.6    4.0   18.1  66.0    1             708   \n",
            "2248    7  71.5  70.6    7.0   15.9  77.0    1             284   \n",
            "2010    3  41.1  38.4    6.5   15.9  48.9    0             583   \n",
            "\n",
            "     NUM_COLLISIONS_predicted  \n",
            "3727         [313.4541015625]  \n",
            "3834      [401.6940002441406]  \n",
            "2859      [526.8675537109375]  \n",
            "3822      [480.9990539550781]  \n",
            "3578      [625.6460571289062]  \n",
            "...                       ...  \n",
            "3537     [232.12440490722656]  \n",
            "2914      [501.8082580566406]  \n",
            "2765       [632.374267578125]  \n",
            "2248      [381.9837646484375]  \n",
            "2010      [569.1399536132812]  \n",
            "\n",
            "[221 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Compare Mean Absolute Error"
      ],
      "metadata": {
        "id": "Jr9V6yn_w-pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Linear Modelling Mean absolute error (MAE) results\")\n",
        "print(\"All variables: \" + str(mean_absolute_error_model_0))\n",
        "print(\"Significant variables: \" + str(mean_absolute_error_model_1))\n",
        "print(\"Significant variables except mean temperature: \" + str(mean_absolute_error_model_2))\n",
        "print(\"Significant variables except mean dewpoint: \" + str(mean_absolute_error_model_3))\n",
        "print(\"Significant variables except mean visibility: \" + str(mean_absolute_error_model_4))\n",
        "print(\"Significant variables except mean wind speed: \" + str(mean_absolute_error_model_5))\n",
        "print(\"Significant variables except maximum sustained wind speed: \" + str(mean_absolute_error_model_6))\n",
        "print(\"Significant variables except maximum wind gust: \" + str(mean_absolute_error_model_7))\n",
        "print(\"Significant variables except day of week: \" + str(mean_absolute_error_model_8))\n",
        "print(\"\")\n",
        "print(\"DNN Modelling Mean absolute error (MAE) results\")\n",
        "print(\"All variables: \" + str(dnn_model_1_results))\n",
        "print(\"Remove mean temperature: \" + str(dnn_model_2_results))\n",
        "print(\"Remove mean dewpoint: \" + str(dnn_model_3_results))\n",
        "print(\"Remove mean sea level pressure: \" + str(dnn_model_4_results))\n",
        "print(\"Remove maximum temperature: \" + str(dnn_model_5_results))\n",
        "print(\"Remove minimum temperature: \" + str(dnn_model_6_results))\n",
        "print(\"Remove reported fog: \" + str(dnn_model_7_results))\n",
        "print(\"Remove mean wind speed: \" + str(dnn_model_8_results))\n",
        "print(\"Remove maximum sustained wind speed: \" + str(dnn_model_9_results))\n",
        "print(\"Remove maximum wind gust: \" + str(dnn_model_10_results))\n",
        "print(\"Remove total precipitation: \" + str(dnn_model_11_results))\n",
        "print(\"Remove mean visibility: \" + str(dnn_model_12_results))\n",
        "print(\"Remove day of week: \" + str(dnn_model_13_results))\n",
        "print(\"All significant variables from linear model: \" + str(dnn_model_14_results))\n",
        "print(\"Remove all variables whose removal decreased MAE: \" + str(dnn_model_15_results))\n",
        "print(\"Remove all variables whose removal decreased MAE AND increase neurons in dense layer: \" + str(dnn_model_16_results))\n",
        "print(\"Remove all variables whose removal decreased MAE AND increase number of layers: \" + str(dnn_model_17_results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJlruUAGw-Ck",
        "outputId": "800ad84e-d5d8-4d92-9d8c-ac0f264ef7ed"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Modelling Mean absolute error (MAE) results\n",
            "All variables: 0.1782083511352539\n",
            "Significant variables: 0.1279795616865158\n",
            "Significant variables except mean temperature: 0.143507719039917\n",
            "Significant variables except mean dewpoint: 0.13664111495018005\n",
            "Significant variables except mean visibility: 0.13224492967128754\n",
            "Significant variables except mean wind speed: 0.13423854112625122\n",
            "Significant variables except maximum sustained wind speed: 0.1345731019973755\n",
            "Significant variables except maximum wind gust: 0.1355268508195877\n",
            "Significant variables except day of week: 0.14146864414215088\n",
            "\n",
            "DNN Modelling Mean absolute error (MAE) results\n",
            "All variables: 0.14019443094730377\n",
            "Remove mean temperature: 0.1428210735321045\n",
            "Remove mean dewpoint: 0.1623493731021881\n",
            "Remove mean sea level pressure: 0.1399620622396469\n",
            "Remove maximum temperature: 0.14132946729660034\n",
            "Remove minimum temperature: 0.13199076056480408\n",
            "Remove reported fog: 0.14263370633125305\n",
            "Remove mean wind speed: 0.13620688021183014\n",
            "Remove maximum sustained wind speed: 0.14368751645088196\n",
            "Remove maximum wind gust: 0.1391804963350296\n",
            "Remove total precipitation: 0.13887201249599457\n",
            "Remove mean visibility: 0.14489327371120453\n",
            "Remove day of week: 0.143428772687912\n",
            "All significant variables from linear model: 0.1338409036397934\n",
            "Remove all variables whose removal decreased MAE: 0.13062207400798798\n",
            "Remove all variables whose removal decreased MAE AND increase neurons in dense layer: 0.13369134068489075\n",
            "Remove all variables whose removal decreased MAE AND increase number of layers: 0.1356942057609558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "It depends on the run, but in this case the lowest mean absolute error was achieved by using the linear model with only the significant variables:\n",
        "*   mean temperature\n",
        "*   mean dewpoint\n",
        "*   mean visibility\n",
        "*   mean wind speed\n",
        "*   maximum sustained wind speed\n",
        "*   maximum wind gust\n",
        "*   total precipitation\n",
        "*   day of week\n",
        "\n",
        "Note that the seed has been set so that the results match the write-up and so removing or changing the seed might produce slightly different results. However, every time I ran these models the lowest mean absolute error was obtained from one of the linear models with either all or some of the significant variables included. Removing any of the significant variables did not greatly reduce the mean absolute error. Prior analysis showed the residuals were not perfectly normally distributed and the mean was not near zero. Therefore, it was thought that a linear model may not be the best predictor and other models, such as DNN should be considered. However, the DNN models failed to produce a lower mean absolute error than the linear models, indicating they are not a better for the data. Increasing either the number of layers or the number of neurons on the best DNN model did not further reduce the mean absolute error. Therefore, the best fitting model remains the linear regression model containing the significant variables listed above.\n",
        "\n",
        "One possible reason that the DNN model did not perform better than the linear model could be that the amount of data was not large enough for the deep neural network to learn from. It was expected that 12 years worth of data would have been sufficient; however, some data was excluded due to missing values. As a result, the DNN may have overfit the data resulting in a lower performing model.\n",
        "\n",
        "It is also possible that even though prior analysis showed that the residuals were not perfectly normally distributed and the mean was not that close to zero, these values were approximate enough that a linear regression model best fits the data. Trying to apply a deep neural network to the data may have introduced unnecessary complexity, when in fact the relationship between variables was close enough to linear all along.\n",
        "\n",
        "Another possibility would be to try looking at other metrics, such as root mean squared error, to compare the models. Mean absolute error was chosen to compare the models due to the number of outliers, but it is possible that other metrics would offer further insight.\n",
        "\n",
        "In conclusion, the results show that the best model to predict the relationship between weather conditions, day of the week, and number of collisions is the linear model using the following variables:\n",
        "*   mean temperature\n",
        "*   mean dewpoint\n",
        "*   mean visibility\n",
        "*   mean wind speed\n",
        "*   maximum sustained wind speed\n",
        "*   maximum wind gust\n",
        "*   total precipitation\n",
        "*   day of week\n",
        "\n",
        "This model produced the lowest mean absolute error of the models tested and was able to predict the number of collisions based on input variables with reasonable accuracy. It is hoped that by inputting this information, the model can be used by the New York City Emergency Services Unit to predict how many collisions are likely to occur on a particular day. In this way, the model can lead to informed decision-making and facilitate planning and optimization of staff resources by the New York City Emergency Services Unit. It is recommended that this model is used in conjunction with the findings from the prior analysis of collision map data so that resources can be directed to where they are most needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "X7V25IaKirVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "Abadi, M. et al. (2015) 'TensorFlow: Large-scale machine learning on heterogeneous systems'. Available at: http://tensorflow.org/\n",
        "\n",
        "Harris, C.R. et al. (2020) 'Array programming with NumPy'. *Nature*, 585, pp.357-362. Available at: https://doi.org/10.1038/s41586-020-2649-2\n",
        "\n",
        "McKinney, W. *et al.* (2010) 'Data structures for statistical computing in python'. *Proceedings of the 9th Python in Science Conference*, pp. 51-56. Available at: https://doi.org/10.25080/Majora-92bf1922-00a\n",
        "\n",
        "New York Police Department (2017). 'NYPD Motor Vehicle Collisions' Available at: https://console.cloud.google.com/marketplace/product/city-of-new-york/nypd-mv-collisions (Accessed: 10 September 2024).\n",
        "\n",
        "NOAA National Centers of Environmental Information (1999) 'Global Surface Summary of the Day - GSOD'. 1.0. (2012-2024). NOAA National Centers for Environmental Information. Available at: https://console.cloud.google.com/marketplace/product/noaa-public/gsod (Accessed: 10 September 2024)."
      ],
      "metadata": {
        "id": "gb9OUpjtis8V"
      }
    }
  ]
}